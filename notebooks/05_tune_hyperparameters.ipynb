{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35591991",
   "metadata": {},
   "source": [
    "This module develops tuned hyperparameters for each machine learning model to be benchmarked.\n",
    "\n",
    "This initial code block defines and configures 16 different ML algorithms for subsequent hyperparameter tuning in the next code cell. The 16 machine learning algorithms are the following:\n",
    "\n",
    "1.  Tree-based:     Decision Tree, Random Forest, Extra Trees, Gradient Boosting, AdaBoost, XGBoost\n",
    "2.  Linear:         Logistic Regression, Ridge Classifier, Stochastic Gradient Descent (SGD) Classifier, Perceptron\n",
    "3.  Kernel-based:   Support Vector Classifier\n",
    "4.  Instance-based: KNeighbors Classifier\n",
    "5.  Probabilistic:  Gaussian Naive Bayes\n",
    "6.  Discriminant:   Linear Discriminant Analysis, Quadratic Discriminant Analysis\n",
    "7.  Neural:         Multi-layer Perceptron Classifier.\n",
    "\n",
    "The configurations are set up in the AVAILABLE_MODELS dictionary. One objective of these configurations is to standardize paramater grids and default parameters for fair comparisons among models, and increase the chance of reproducible results. Another objective was to load models at runtime, rather than having to hard-code all the imports. This should make scaling up to more ML models less painful.\n",
    "\n",
    "Each model's configuration inclues:\n",
    "\n",
    "1.  Class and module:   For dynamic importing of its libraries\n",
    "2.  Label requirements: Whether model needs numeric vs. text labels\n",
    "3.  Default parameters: Base settings (random_state, n_jobs, etc.)\n",
    "4.  Parameter grid:     Hyperparameters to tune and their value choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6efa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 1. ESTABLISH TAXONOMY OF ML MODELS\n",
    "# -------------------------------------------\n",
    "\n",
    "MODEL_FAMILIES = {\n",
    "    \"Tree-Based Models\": [\n",
    "        (1, \"DecisionTree\"),\n",
    "        (2, \"RandomForest\"),\n",
    "        (3, \"ExtraTrees\"),\n",
    "        (4, \"GradientBoosting\"),\n",
    "        (5, \"AdaBoost\"),\n",
    "        (6, \"XGBoost\")\n",
    "    ],\n",
    "    \"Linear Models\": [\n",
    "        (7, \"LogisticRegression\"),\n",
    "        (8, \"RidgeClassifier\"),\n",
    "        (9, \"SGDClassifier\"),\n",
    "        (10, \"Perceptron\")\n",
    "    ],\n",
    "    \"Kernel-Based Models\": [\n",
    "        (11, \"SVC\")\n",
    "    ],\n",
    "    \"Instance-Based Models\": [\n",
    "        (12, \"Kneighbors\")\n",
    "    ],\n",
    "    \"Probabilistic Models\": [\n",
    "        (13, \"GaussianNB\")\n",
    "    ],\n",
    "    \"Discriminant Models\": [\n",
    "        (14, \"LDA\"),\n",
    "        (15, \"QDA\")\n",
    "    ],\n",
    "    \"Neural Models\": [\n",
    "        (16, \"MLP\")\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69276a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default parameters in AVAILABLE_MODELS (set in default_params for each model):\n",
      "\n",
      "DecisionTreeClassifier:\n",
      "   random_state = 42\n",
      "\n",
      "RandomForestClassifier:\n",
      "   random_state = 42\n",
      "   n_jobs = -1\n",
      "\n",
      "ExtraTreesClassifier:\n",
      "   random_state = 42\n",
      "   n_jobs = -1\n",
      "\n",
      "GradientBoostingClassifier:\n",
      "   random_state = 42\n",
      "   subsample = 0.8\n",
      "   max_depth = 5\n",
      "   max_features = sqrt\n",
      "\n",
      "AdaBoostClassifier:\n",
      "   random_state = 42\n",
      "\n",
      "XGBClassifier:\n",
      "   random_state = 42\n",
      "   use_label_encoder = False\n",
      "   eval_metric = mlogloss\n",
      "   n_jobs = -1\n",
      "\n",
      "LogisticRegression:\n",
      "   random_state = 42\n",
      "   n_jobs = -1\n",
      "   max_iter = 1000\n",
      "\n",
      "RidgeClassifier:\n",
      "\n",
      "SGDClassifier:\n",
      "   random_state = 42\n",
      "   n_jobs = -1\n",
      "\n",
      "Perceptron:\n",
      "   random_state = 42\n",
      "   max_iter = 1000\n",
      "   n_jobs = -1\n",
      "\n",
      "KNeighborsClassifier:\n",
      "   n_jobs = -1\n",
      "\n",
      "SVC:\n",
      "   random_state = 42\n",
      "   probability = True\n",
      "\n",
      "GaussianNB:\n",
      "\n",
      "LinearDiscriminantAnalysis:\n",
      "\n",
      "QuadraticDiscriminantAnalysis:\n",
      "\n",
      "MLPClassifier:\n",
      "   random_state = 42\n",
      "   max_iter = 1000\n",
      "\n",
      "\n",
      "âœ… All default_params are valid across all AVAILABLE_MODELS.\n",
      "Default values for hyperparameters listed in param_grid:\n",
      "\n",
      "DecisionTreeClassifier:\n",
      "   criterion: gini\n",
      "   max_depth: None\n",
      "   min_samples_split: 2\n",
      "   min_samples_leaf: 1\n",
      "\n",
      "RandomForestClassifier:\n",
      "   n_estimators: 100\n",
      "   max_depth: None\n",
      "   min_samples_split: 2\n",
      "   min_samples_leaf: 1\n",
      "\n",
      "ExtraTreesClassifier:\n",
      "   n_estimators: 100\n",
      "   max_depth: None\n",
      "   min_samples_split: 2\n",
      "\n",
      "GradientBoostingClassifier:\n",
      "   n_estimators: 100\n",
      "   learning_rate: 0.1\n",
      "   max_depth: 5\n",
      "\n",
      "AdaBoostClassifier:\n",
      "   n_estimators: 50\n",
      "   learning_rate: 1.0\n",
      "\n",
      "XGBClassifier:\n",
      "   n_estimators: None\n",
      "   max_depth: None\n",
      "   learning_rate: None\n",
      "   subsample: None\n",
      "\n",
      "LogisticRegression:\n",
      "   C: 1.0\n",
      "   penalty: l2\n",
      "   solver: lbfgs\n",
      "\n",
      "RidgeClassifier:\n",
      "   alpha: 1.0\n",
      "   solver: auto\n",
      "\n",
      "SGDClassifier:\n",
      "   loss: hinge\n",
      "   alpha: 0.0001\n",
      "   penalty: l2\n",
      "\n",
      "Perceptron:\n",
      "   penalty: None\n",
      "   alpha: 0.0001\n",
      "\n",
      "KNeighborsClassifier:\n",
      "   n_neighbors: 5\n",
      "   weights: uniform\n",
      "   metric: minkowski\n",
      "\n",
      "SVC:\n",
      "   C: 1.0\n",
      "   kernel: rbf\n",
      "   gamma: scale\n",
      "\n",
      "LinearDiscriminantAnalysis:\n",
      "   solver: svd\n",
      "   shrinkage: None\n",
      "\n",
      "QuadraticDiscriminantAnalysis:\n",
      "   reg_param: 0.0\n",
      "\n",
      "MLPClassifier:\n",
      "   hidden_layer_sizes: (100,)\n",
      "   activation: relu\n",
      "   solver: adam\n",
      "   alpha: 0.0001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 2. DEFINE MODELS, DEFAULT HYPERPARAMETERS, AND TO-BE-TUNED HYPERPARAMETERS\n",
    "# -------------------------------------------\n",
    "\n",
    "\n",
    "# Import\n",
    "import pandas as pd\n",
    "import json\n",
    "import importlib\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output. Many arise during hyperparameter tuning.\n",
    "warnings.filterwarnings(\"ignore\")  \n",
    "\n",
    "# Define this project's file locations.\n",
    "# This notebook uses a centralized config.py file for all path management.\n",
    "\n",
    "# Import config paths\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import CURATED_DATA_DIR, TUNED_MODELS_DIR\n",
    "\n",
    "# Define paths specific to this module\n",
    "curated_path = CURATED_DATA_DIR / \"DryBean_curated.parquet\"\n",
    "tuned_models_dir = TUNED_MODELS_DIR\n",
    "tuned_models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Load scaled and encoded dataset -- again.\n",
    "# Loading from persistant storage (e.g., hard/SSD drive) is intended to ensure modularity and reliability.\n",
    "df = pd.read_parquet(curated_path)\n",
    "X = df.drop(columns=[\"label\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Define the 16 models to allow for dynamic importing of their libraries in the next code cell.\n",
    "# Configure each model for hyperparameter tuning. This requires defining: \n",
    "#    1. Default parameters (e.g., random_state, n_jobs)\n",
    "#    2. Search type (\"grid\" for GridSearchCV, the chosen method in next code cell)\n",
    "#    3. Scoring metric to assess how will a hyperparameter does (\"accuracy\") \n",
    "#    4. The specific hyperparameters to tune in the next code block (in the \"param_grid\" dictionary).\n",
    "# Each model is defined in the AVAILABLE_MODELS dictionary with its class, module, and hyperparameter grid.\n",
    "\n",
    "\n",
    "AVAILABLE_MODELS = {\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        \"class\": \"DecisionTreeClassifier\",\n",
    "        \"module\": \"sklearn.tree\",\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {\"random_state\": 42},\n",
    "        \"param_grid\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": [None, 5, 10, 20],\n",
    "            \"min_samples_split\": [2, 5],\n",
    "            \"min_samples_leaf\": [1, 3]\n",
    "        }\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"class\": \"RandomForestClassifier\",\n",
    "        \"module\": \"sklearn.ensemble\",\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {\"random_state\": 42, \"n_jobs\": -1},\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"max_depth\": [None, 10, 20],\n",
    "            \"min_samples_split\": [2, 5],\n",
    "            \"min_samples_leaf\": [1, 3]\n",
    "        }\n",
    "    },\n",
    "    \"ExtraTreesClassifier\": {\n",
    "        \"class\": \"ExtraTreesClassifier\",\n",
    "        \"module\": \"sklearn.ensemble\",\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {\"random_state\": 42, \"n_jobs\": -1},\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"max_depth\": [None, 10, 20],\n",
    "            \"min_samples_split\": [2, 5]\n",
    "        }\n",
    "    },\n",
    "    \"GradientBoostingClassifier\": {\n",
    "        \"class\": \"GradientBoostingClassifier\",\n",
    "        \"module\": \"sklearn.ensemble\",\n",
    "        \"subsample\": 0.8,\n",
    "        \"max_depth\": 5,\n",
    "        \"max_features\": \"sqrt\",\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {\"random_state\": 42, \"subsample\": 0.8,\n",
    "            \"max_depth\": 5,\n",
    "            \"max_features\": \"sqrt\"\n",
    "        },\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"max_depth\": [3, 5]\n",
    "        }\n",
    "    },\n",
    "    \"AdaBoostClassifier\": {\n",
    "        \"class\": \"AdaBoostClassifier\",\n",
    "        \"module\": \"sklearn.ensemble\",\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {\"random_state\": 42},\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"learning_rate\": [0.5, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"XGBClassifier\": {\n",
    "        \"class\": \"XGBClassifier\",\n",
    "        \"module\": \"xgboost\",\n",
    "        \"requires_numeric_labels\": True,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {\n",
    "            \"random_state\": 42,\n",
    "            \"use_label_encoder\": False,\n",
    "            \"eval_metric\": \"mlogloss\",\n",
    "            \"n_jobs\": -1\n",
    "        },\n",
    "        \"param_grid\": {\n",
    "            \"n_estimators\": [50, 100],\n",
    "            \"max_depth\": [3, 6],\n",
    "            \"learning_rate\": [0.1, 0.2],\n",
    "            \"subsample\": [0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"class\": \"LogisticRegression\",\n",
    "        \"module\": \"sklearn.linear_model\",\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {\"random_state\": 42, \"n_jobs\": -1, \"max_iter\": 1000},\n",
    "        \"param_grid\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "            \"penalty\": [\"l2\"],\n",
    "            \"solver\": [\"lbfgs\", \"liblinear\"]\n",
    "        }\n",
    "    },\n",
    "    \"RidgeClassifier\": {\n",
    "        \"class\": \"RidgeClassifier\",\n",
    "        \"module\": \"sklearn.linear_model\",\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {},\n",
    "        \"param_grid\": {\n",
    "            \"alpha\": [0.1, 1.0, 10.0],\n",
    "            \"solver\": [\"auto\", \"sparse_cg\"]\n",
    "        }\n",
    "    },\n",
    "    \"SGDClassifier\": {\n",
    "        \"class\": \"SGDClassifier\",\n",
    "        \"module\": \"sklearn.linear_model\",\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {\"random_state\": 42, \"n_jobs\": -1},\n",
    "        \"param_grid\": {\n",
    "            \"loss\": [\"hinge\", \"log_loss\"],\n",
    "            \"alpha\": [0.0001, 0.001],\n",
    "            \"penalty\": [\"l2\", \"l1\"]\n",
    "        }\n",
    "    },\n",
    "    \"Perceptron\": {\n",
    "        \"class\": \"Perceptron\",\n",
    "        \"module\": \"sklearn.linear_model\",\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {\"random_state\": 42, \"max_iter\": 1000, \"n_jobs\": -1},\n",
    "        \"param_grid\": {\n",
    "            \"penalty\": [\"l2\", \"elasticnet\", None],\n",
    "            \"alpha\": [0.0001, 0.001]\n",
    "        }\n",
    "    },\n",
    "    \"KNeighborsClassifier\": {\n",
    "        \"class\": \"KNeighborsClassifier\",\n",
    "        \"module\": \"sklearn.neighbors\",\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {\"n_jobs\": -1},\n",
    "        \"param_grid\": {\n",
    "            \"n_neighbors\": [3, 5, 7],\n",
    "            \"weights\": [\"uniform\", \"distance\"],\n",
    "            \"metric\": [\"euclidean\", \"manhattan\"]\n",
    "        }\n",
    "    },\n",
    "    \"SVC\": {\n",
    "        \"class\": \"SVC\",\n",
    "        \"module\": \"sklearn.svm\",\n",
    "        \"random_state\": 42,\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {\"random_state\": 42, \"probability\": True},\n",
    "        \"param_grid\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "            \"kernel\": [\"linear\", \"rbf\"],\n",
    "            \"gamma\": [\"scale\", \"auto\"]\n",
    "        }\n",
    "    },\n",
    "    \"GaussianNB\": {\n",
    "        \"class\": \"GaussianNB\",\n",
    "        \"module\": \"sklearn.naive_bayes\",\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {},\n",
    "        \"param_grid\": {}\n",
    "    },\n",
    "    \"LinearDiscriminantAnalysis\": {\n",
    "        \"class\": \"LinearDiscriminantAnalysis\",\n",
    "        \"module\": \"sklearn.discriminant_analysis\",\n",
    "        \"requires_numeric_labels\": False,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {},\n",
    "        \"param_grid\": {\n",
    "            \"solver\": [\"svd\", \"lsqr\"],\n",
    "            \"shrinkage\": [None, \"auto\"]\n",
    "        }\n",
    "    },\n",
    "    \"QuadraticDiscriminantAnalysis\": {\n",
    "    \"class\": \"QuadraticDiscriminantAnalysis\",\n",
    "    \"module\": \"sklearn.discriminant_analysis\",\n",
    "    \"requires_numeric_labels\": False,\n",
    "    \"search_type\": \"grid\",\n",
    "    \"scoring\": \"accuracy\",\n",
    "    \"default_params\": {},\n",
    "    \"param_grid\": {\n",
    "        \"reg_param\": [0.0, 0.1, 0.5]\n",
    "    }\n",
    "    },\n",
    "    \"MLPClassifier\": {\n",
    "        \"class\": \"MLPClassifier\",\n",
    "        \"module\": \"sklearn.neural_network\",\n",
    "        \"requires_numeric_labels\": True,\n",
    "        \"search_type\": \"grid\",\n",
    "        \"scoring\": \"accuracy\",\n",
    "        \"default_params\": {\"random_state\": 42, \"max_iter\": 1000},\n",
    "        \"param_grid\": {\n",
    "            \"hidden_layer_sizes\": [(50,), (100,), (50, 50)],\n",
    "            \"activation\": [\"relu\", \"tanh\"],\n",
    "            \"solver\": [\"adam\", \"lbfgs\"],\n",
    "            \"alpha\": [0.0001, 0.001, 0.01]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Default parameters in AVAILABLE_MODELS (set in default_params for each model):\\n\")\n",
    "\n",
    "# Show what default parameters will be used (informational only).\n",
    "for name, config in AVAILABLE_MODELS.items():\n",
    "    if \"default_params\" in config:\n",
    "        print(f\"{name}:\")\n",
    "        for k, v in config[\"default_params\"].items():\n",
    "            print(f\"   {k} = {v}\")\n",
    "        print()\n",
    "\n",
    "all_valid = True\n",
    "\n",
    "# Instantiate each model with its default parameters.\n",
    "# Before doing so, confirm that all default parameters for each model are valid for that model before tuning in the next cell.\n",
    "# Doing so, and correcting any parameter errors, will avoid runtime errors during hyperparameter tuning (which takes buku time). \n",
    "for name, config in AVAILABLE_MODELS.items():\n",
    "    module = importlib.import_module(config[\"module\"])\n",
    "    model_class = getattr(module, config[\"class\"])\n",
    "    try:\n",
    "        model_class(**config[\"default_params\"])\n",
    "    except TypeError as e:\n",
    "        all_valid = False\n",
    "        print(f\"âŒ {name} failed: {e}\")\n",
    "\n",
    "if all_valid:\n",
    "    print(\"\\nâœ… All default_params are valid across all AVAILABLE_MODELS.\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Default values for hyperparameters listed in param_grid:\\n\")\n",
    "\n",
    "for model_name, config in AVAILABLE_MODELS.items():\n",
    "    param_grid = config.get(\"param_grid\", {})\n",
    "    if not param_grid:\n",
    "        continue\n",
    "    module = importlib.import_module(config[\"module\"])\n",
    "    model_class = getattr(module, config[\"class\"])\n",
    "    default_instance = model_class(**config.get(\"default_params\", {}))\n",
    "    default_params = default_instance.get_params()\n",
    "    print(f\"{model_name}:\")\n",
    "    for param in param_grid:\n",
    "        value = default_params.get(param, \"(not set)\")\n",
    "        print(f\"   {param}: {value}\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f882de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default parameters in AVAILABLE_MODELS (set in default_params for each model):\n",
      "\n",
      "--- Tree-Based Models ---\n",
      " 1. DecisionTreeClassifier:\n",
      "      random_state = 42\n",
      "\n",
      " 2. RandomForestClassifier:\n",
      "      random_state = 42\n",
      "      n_jobs = -1\n",
      "\n",
      " 3. ExtraTreesClassifier:\n",
      "      random_state = 42\n",
      "      n_jobs = -1\n",
      "\n",
      " 4. GradientBoostingClassifier:\n",
      "      random_state = 42\n",
      "      subsample = 0.8\n",
      "      max_depth = 5\n",
      "      max_features = sqrt\n",
      "\n",
      " 5. AdaBoostClassifier:\n",
      "      random_state = 42\n",
      "\n",
      " 6. XGBClassifier:\n",
      "      random_state = 42\n",
      "      use_label_encoder = False\n",
      "      eval_metric = mlogloss\n",
      "      n_jobs = -1\n",
      "\n",
      "\n",
      "--- Linear Models ---\n",
      " 7. LogisticRegression:\n",
      "      random_state = 42\n",
      "      n_jobs = -1\n",
      "      max_iter = 1000\n",
      "\n",
      " 8. RidgeClassifier: (no default params)\n",
      "\n",
      " 9. SGDClassifier:\n",
      "      random_state = 42\n",
      "      n_jobs = -1\n",
      "\n",
      "10. Perceptron:\n",
      "      random_state = 42\n",
      "      max_iter = 1000\n",
      "      n_jobs = -1\n",
      "\n",
      "\n",
      "--- Kernel-Based Models ---\n",
      "11. SVC:\n",
      "      random_state = 42\n",
      "      probability = True\n",
      "\n",
      "\n",
      "--- Instance-Based Models ---\n",
      "12. KNeighborsClassifier:\n",
      "      n_jobs = -1\n",
      "\n",
      "\n",
      "--- Probabilistic Models ---\n",
      "13. GaussianNB: (no default params)\n",
      "\n",
      "\n",
      "--- Discriminant Models ---\n",
      "14. LinearDiscriminantAnalysis: (no default params)\n",
      "\n",
      "15. QuadraticDiscriminantAnalysis: (no default params)\n",
      "\n",
      "\n",
      "--- Neural Models ---\n",
      "16. MLPClassifier:\n",
      "      random_state = 42\n",
      "      max_iter = 1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 3. CREATE JSON FILE WITH MODEL CONFIGURTIONS AND ORDERING\n",
    "# -------------------------------------------\n",
    "\n",
    "# This code creates a .json file that contains the model configurations and their ordering\n",
    "# based on the MODEL_FAMILIES dictionary. The json file is used in the notebook 07 \n",
    "# to display the models and their default parameters in a structured way.\n",
    "\n",
    "\n",
    "\n",
    "print(\"Default parameters in AVAILABLE_MODELS (set in default_params for each model):\\n\")\n",
    "\n",
    "# Create ordered list based on MODEL_FAMILIES\n",
    "ordered_models = []\n",
    "for family_name, models in MODEL_FAMILIES.items():\n",
    "    for order_num, model_short_name in models:\n",
    "        # Map short names to full classifier names\n",
    "        model_mapping = {\n",
    "            \"RandomForest\": \"RandomForestClassifier\",\n",
    "            \"ExtraTrees\": \"ExtraTreesClassifier\", \n",
    "            \"GradientBoosting\": \"GradientBoostingClassifier\",\n",
    "            \"AdaBoost\": \"AdaBoostClassifier\",\n",
    "            \"DecisionTree\": \"DecisionTreeClassifier\",\n",
    "            \"XGBoost\": \"XGBClassifier\",\n",
    "            \"LogisticRegression\": \"LogisticRegression\",\n",
    "            \"RidgeClassifier\": \"RidgeClassifier\",\n",
    "            \"SGDClassifier\": \"SGDClassifier\", \n",
    "            \"Perceptron\": \"Perceptron\",\n",
    "            \"SVC\": \"SVC\",\n",
    "            \"Kneighbors\": \"KNeighborsClassifier\",\n",
    "            \"GaussianNB\": \"GaussianNB\",\n",
    "            \"LDA\": \"LinearDiscriminantAnalysis\",\n",
    "            \"QDA\": \"QuadraticDiscriminantAnalysis\",\n",
    "            \"MLP\": \"MLPClassifier\"\n",
    "        }\n",
    "        \n",
    "        full_name = model_mapping.get(model_short_name)\n",
    "        if full_name and full_name in AVAILABLE_MODELS:\n",
    "            ordered_models.append((order_num, full_name, family_name))\n",
    "\n",
    "# Save model configuration (so these dictionaries can be accessed in notebook 07)\n",
    "model_config_path = tuned_models_dir / \"model_config.json\"\n",
    "model_config = {\n",
    "    'MODEL_FAMILIES': MODEL_FAMILIES,\n",
    "    'model_mapping': model_mapping\n",
    "}\n",
    "with open(model_config_path, \"w\") as f:\n",
    "    json.dump(model_config, f, indent=4)\n",
    "\n",
    "# Sort by order number and display\n",
    "ordered_models.sort(key=lambda x: x[0])\n",
    "\n",
    "current_family = None\n",
    "for order_num, model_name, family_name in ordered_models:\n",
    "    # Print family header when family changes\n",
    "    if family_name != current_family:\n",
    "        if current_family is not None:\n",
    "            print()  # Extra line between families\n",
    "        print(f\"--- {family_name} ---\")\n",
    "        current_family = family_name\n",
    "    \n",
    "    config = AVAILABLE_MODELS[model_name]\n",
    "    if \"default_params\" in config and config[\"default_params\"]:\n",
    "        print(f\"{order_num:2d}. {model_name}:\")\n",
    "        for k, v in config[\"default_params\"].items():\n",
    "            print(f\"      {k} = {v}\")\n",
    "    else:\n",
    "        print(f\"{order_num:2d}. {model_name}: (no default params)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c01ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tuning DecisionTreeClassifier...\n",
      " DecisionTreeClassifier done â€” Best CV score: 0.9081\n",
      "\n",
      " Tuning RandomForestClassifier...\n",
      " DecisionTreeClassifier done â€” Best CV score: 0.9081\n",
      "\n",
      " Tuning RandomForestClassifier...\n",
      " RandomForestClassifier done â€” Best CV score: 0.9256\n",
      "\n",
      " Tuning ExtraTreesClassifier...\n",
      " RandomForestClassifier done â€” Best CV score: 0.9256\n",
      "\n",
      " Tuning ExtraTreesClassifier...\n",
      " ExtraTreesClassifier done â€” Best CV score: 0.9229\n",
      "\n",
      " Tuning GradientBoostingClassifier...\n",
      " ExtraTreesClassifier done â€” Best CV score: 0.9229\n",
      "\n",
      " Tuning GradientBoostingClassifier...\n",
      " GradientBoostingClassifier done â€” Best CV score: 0.9269\n",
      "\n",
      " Tuning AdaBoostClassifier...\n",
      " GradientBoostingClassifier done â€” Best CV score: 0.9269\n",
      "\n",
      " Tuning AdaBoostClassifier...\n",
      " AdaBoostClassifier done â€” Best CV score: 0.8441\n",
      "\n",
      " Tuning XGBClassifier...\n",
      " AdaBoostClassifier done â€” Best CV score: 0.8441\n",
      "\n",
      " Tuning XGBClassifier...\n",
      " XGBClassifier done â€” Best CV score: 0.9302\n",
      "\n",
      " Tuning LogisticRegression...\n",
      " XGBClassifier done â€” Best CV score: 0.9302\n",
      "\n",
      " Tuning LogisticRegression...\n",
      " LogisticRegression done â€” Best CV score: 0.9243\n",
      "\n",
      " Tuning RidgeClassifier...\n",
      " RidgeClassifier done â€” Best CV score: 0.9026\n",
      "\n",
      " Tuning SGDClassifier...\n",
      " LogisticRegression done â€” Best CV score: 0.9243\n",
      "\n",
      " Tuning RidgeClassifier...\n",
      " RidgeClassifier done â€” Best CV score: 0.9026\n",
      "\n",
      " Tuning SGDClassifier...\n",
      " SGDClassifier done â€” Best CV score: 0.9190\n",
      "\n",
      " Tuning Perceptron...\n",
      " SGDClassifier done â€” Best CV score: 0.9190\n",
      "\n",
      " Tuning Perceptron...\n",
      " Perceptron done â€” Best CV score: 0.9021\n",
      "\n",
      " Tuning KNeighborsClassifier...\n",
      " Perceptron done â€” Best CV score: 0.9021\n",
      "\n",
      " Tuning KNeighborsClassifier...\n",
      " KNeighborsClassifier done â€” Best CV score: 0.9250\n",
      "\n",
      " Tuning SVC...\n",
      " KNeighborsClassifier done â€” Best CV score: 0.9250\n",
      "\n",
      " Tuning SVC...\n",
      " SVC done â€” Best CV score: 0.9314\n",
      "\n",
      " Tuning GaussianNB...\n",
      "âš ï¸  No hyperparameter grid â€” using default params. Score: 0.8974\n",
      " GaussianNB done â€” Best CV score: 0.8974\n",
      "\n",
      " Tuning LinearDiscriminantAnalysis...\n",
      " LinearDiscriminantAnalysis done â€” Best CV score: 0.9046\n",
      "\n",
      " Tuning QuadraticDiscriminantAnalysis...\n",
      " SVC done â€” Best CV score: 0.9314\n",
      "\n",
      " Tuning GaussianNB...\n",
      "âš ï¸  No hyperparameter grid â€” using default params. Score: 0.8974\n",
      " GaussianNB done â€” Best CV score: 0.8974\n",
      "\n",
      " Tuning LinearDiscriminantAnalysis...\n",
      " LinearDiscriminantAnalysis done â€” Best CV score: 0.9046\n",
      "\n",
      " Tuning QuadraticDiscriminantAnalysis...\n",
      " QuadraticDiscriminantAnalysis done â€” Best CV score: 0.9194\n",
      "\n",
      " Tuning MLPClassifier...\n",
      " QuadraticDiscriminantAnalysis done â€” Best CV score: 0.9194\n",
      "\n",
      " Tuning MLPClassifier...\n",
      " MLPClassifier done â€” Best CV score: 0.9338\n",
      "\n",
      "Best parameters saved to: C:\\Misc\\ml_benchmark\\outputs\\tuned_models\\best_params.json\n",
      " MLPClassifier done â€” Best CV score: 0.9338\n",
      "\n",
      "Best parameters saved to: C:\\Misc\\ml_benchmark\\outputs\\tuned_models\\best_params.json\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 4. TUNE HYPERPARAMETERS AND SAVE RESULTS\n",
    "# -------------------------------------------\n",
    "\n",
    "# This code cell can take approximately 10 minutes to run.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from joblib import dump\n",
    "import importlib\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Encode labels once (for models that require numeric targets like XGBoost and MLP)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Collect best parameters here\n",
    "best_params_summary = {}\n",
    "\n",
    "# Below is where the param_grid entry in each modelâ€™s dictionary entry in the prior cell gets\n",
    "# systematically explored. For example, for RandomForestClassifier tuning tests, there are\n",
    "# 3x2Ã—2Ã—2 = 24 parameter combinations defined in its dictionary entry, or 24 possible\n",
    "# combinations.  Each combination is evaluated with 5-fold stratified CV, meaning there\n",
    "# are 120 total model fits for the Random Forest algorithm.\n",
    "# Results from each model are saved to an individual CSV file.\n",
    "# A consolidated summary of all tuned parameters is saved in the best_params.json file.\n",
    "\n",
    "\n",
    "\n",
    "for model_name, config in AVAILABLE_MODELS.items():\n",
    "    print(f\"\\n Tuning {model_name}...\")\n",
    "\n",
    "    # Load the model class dynamically\n",
    "    module = importlib.import_module(config[\"module\"])\n",
    "    model_class = getattr(module, config[\"class\"])\n",
    "    model = model_class(**config[\"default_params\"])\n",
    "\n",
    "    # Use numeric labels if model needs them\n",
    "    y_target = y_encoded if config[\"requires_numeric_labels\"] else y\n",
    "\n",
    "    # Select cross-validation strategy\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Select search method\n",
    "    search_cls = GridSearchCV if config[\"search_type\"] == \"grid\" else RandomizedSearchCV\n",
    "    param_grid = config[\"param_grid\"]\n",
    "\n",
    "    # If no hyperparameters to tune, skip GridSearch\n",
    "    if not param_grid:\n",
    "        model.fit(X, y_target)\n",
    "        best_score = model.score(X, y_target)\n",
    "        best_params = config[\"default_params\"]\n",
    "\n",
    "        # Inject reproducibility params if supported\n",
    "        if \"random_state\" in model.get_params():\n",
    "            best_params[\"random_state\"] = 42\n",
    "        if \"n_jobs\" in model.get_params():\n",
    "            best_params[\"n_jobs\"] = -1\n",
    "\n",
    "        print(f\"âš ï¸  No hyperparameter grid â€” using default params. Score: {best_score:.4f}\")\n",
    "    else:\n",
    "        search = search_cls(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            scoring=config[\"scoring\"],\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        search.fit(X, y_target)\n",
    "        best_model = search.best_estimator_\n",
    "        best_score = search.best_score_\n",
    "        best_params = search.best_params_    # This is where the combination of hyperparameters yielding the best score is stored.\n",
    "\n",
    "        # Inject reproducibility params if supported to:\n",
    "        #   1. Ensure consistent results across runs\n",
    "        #   2. Allow multi-threading if the ML model supports it.\n",
    "        if \"random_state\" in best_model.get_params():\n",
    "            best_params[\"random_state\"] = 42\n",
    "        if \"n_jobs\" in best_model.get_params():\n",
    "            best_params[\"n_jobs\"] = -1\n",
    "\n",
    "        # Save full CV search results\n",
    "        results_df = pd.DataFrame(search.cv_results_)\n",
    "        results_path = tuned_models_dir / f\"cv_results_{model_name}.csv\"\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "\n",
    "    # Add to summary\n",
    "    best_params_summary[model_name] = {\n",
    "        \"best_params\": best_params,\n",
    "        \"best_score\": round(best_score, 4),\n",
    "        \"search_type\": config[\"search_type\"],\n",
    "        \"scoring\": config[\"scoring\"]\n",
    "    }\n",
    "\n",
    "    print(f\" {model_name} done â€” Best CV score: {best_score:.4f}\")\n",
    "\n",
    "# Save best parameter summary\n",
    "summary_path = tuned_models_dir / \"best_params.json\"\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(best_params_summary, f, indent=4)\n",
    "\n",
    "print(f\"\\nBest parameters saved to: {summary_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a7cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Models that accept random_state:\n",
      "   â€¢ DecisionTreeClassifier\n",
      "   â€¢ RandomForestClassifier\n",
      "   â€¢ ExtraTreesClassifier\n",
      "   â€¢ GradientBoostingClassifier\n",
      "   â€¢ AdaBoostClassifier\n",
      "   â€¢ XGBClassifier\n",
      "   â€¢ LogisticRegression\n",
      "   â€¢ RidgeClassifier\n",
      "   â€¢ SGDClassifier\n",
      "   â€¢ Perceptron\n",
      "   â€¢ SVC\n",
      "   â€¢ MLPClassifier\n",
      "\n",
      "âœ… Models that accept n_jobs:\n",
      "   â€¢ RandomForestClassifier\n",
      "   â€¢ ExtraTreesClassifier\n",
      "   â€¢ XGBClassifier\n",
      "   â€¢ LogisticRegression\n",
      "   â€¢ SGDClassifier\n",
      "   â€¢ Perceptron\n",
      "   â€¢ KNeighborsClassifier\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 5. LOG WHICH MODELS ACCEPT FIXED PARAMS\n",
    "# -------------------------------------------\n",
    "\n",
    "# Some of the models can accept these two fixed parameter assignments: random_state=42 and/or n_jobs=-1\n",
    "# These parameters were included in the .json file parameters file, above.\n",
    "# This code cell confirms which models can, and did, accept one or both of these fixed parameters.\n",
    "\n",
    "\n",
    "# Optional: verify support for fixed reproducibility params\n",
    "supported_random = []\n",
    "supported_n_jobs = []\n",
    "\n",
    "for model_name, meta in AVAILABLE_MODELS.items():\n",
    "    module = importlib.import_module(meta[\"module\"])\n",
    "    model_class = getattr(module, meta[\"class\"])\n",
    "    params = model_class().get_params()\n",
    "\n",
    "    if \"random_state\" in params:\n",
    "        supported_random.append(model_name)\n",
    "    if \"n_jobs\" in params:\n",
    "        supported_n_jobs.append(model_name)\n",
    "\n",
    "print(\"\\nâœ… Models that accept random_state:\")\n",
    "for m in supported_random:\n",
    "    print(f\"   â€¢ {m}\")\n",
    "\n",
    "print(\"\\nâœ… Models that accept n_jobs:\")\n",
    "for m in supported_n_jobs:\n",
    "    print(f\"   â€¢ {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb4559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL TUNED PARAMETERS FOR EACH MODEL\n",
      "============================================================\n",
      "\n",
      "ğŸ”§ DecisionTreeClassifier:\n",
      "   Best CV Score: 0.9081\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      ccp_alpha: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      class_weight: None (ğŸ“‹ DEFAULT)\n",
      "      criterion: gini (ğŸ¯ TUNED)\n",
      "      max_depth: 10 (ğŸ¯ TUNED)\n",
      "      max_features: None (ğŸ“‹ DEFAULT)\n",
      "      max_leaf_nodes: None (ğŸ“‹ DEFAULT)\n",
      "      min_impurity_decrease: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      min_samples_leaf: 1 (ğŸ¯ TUNED)\n",
      "      min_samples_split: 5 (ğŸ¯ TUNED)\n",
      "      min_weight_fraction_leaf: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      monotonic_cst: None (ğŸ“‹ DEFAULT)\n",
      "      random_state: 42 (âš¡ REPRO)\n",
      "      splitter: best (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ RandomForestClassifier:\n",
      "   Best CV Score: 0.9256\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      bootstrap: True (ğŸ“‹ DEFAULT)\n",
      "      ccp_alpha: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      class_weight: None (ğŸ“‹ DEFAULT)\n",
      "      criterion: gini (ğŸ“‹ DEFAULT)\n",
      "      max_depth: 20 (ğŸ¯ TUNED)\n",
      "      max_features: sqrt (ğŸ“‹ DEFAULT)\n",
      "      max_leaf_nodes: None (ğŸ“‹ DEFAULT)\n",
      "      max_samples: None (ğŸ“‹ DEFAULT)\n",
      "      min_impurity_decrease: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      min_samples_leaf: 1 (ğŸ¯ TUNED)\n",
      "      min_samples_split: 2 (ğŸ¯ TUNED)\n",
      "      min_weight_fraction_leaf: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      monotonic_cst: None (ğŸ“‹ DEFAULT)\n",
      "      n_estimators: 50 (ğŸ¯ TUNED)\n",
      "      n_jobs: -1 (âš¡ REPRO)\n",
      "      oob_score: False (ğŸ“‹ DEFAULT)\n",
      "      random_state: 42 (âš¡ REPRO)\n",
      "      verbose: 0 (ğŸ“‹ DEFAULT)\n",
      "      warm_start: False (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ ExtraTreesClassifier:\n",
      "   Best CV Score: 0.9229\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      bootstrap: False (ğŸ“‹ DEFAULT)\n",
      "      ccp_alpha: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      class_weight: None (ğŸ“‹ DEFAULT)\n",
      "      criterion: gini (ğŸ“‹ DEFAULT)\n",
      "      max_depth: None (ğŸ¯ TUNED)\n",
      "      max_features: sqrt (ğŸ“‹ DEFAULT)\n",
      "      max_leaf_nodes: None (ğŸ“‹ DEFAULT)\n",
      "      max_samples: None (ğŸ“‹ DEFAULT)\n",
      "      min_impurity_decrease: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      min_samples_leaf: 1 (ğŸ“‹ DEFAULT)\n",
      "      min_samples_split: 5 (ğŸ¯ TUNED)\n",
      "      min_weight_fraction_leaf: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      monotonic_cst: None (ğŸ“‹ DEFAULT)\n",
      "      n_estimators: 100 (ğŸ¯ TUNED)\n",
      "      n_jobs: -1 (âš¡ REPRO)\n",
      "      oob_score: False (ğŸ“‹ DEFAULT)\n",
      "      random_state: 42 (âš¡ REPRO)\n",
      "      verbose: 0 (ğŸ“‹ DEFAULT)\n",
      "      warm_start: False (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ GradientBoostingClassifier:\n",
      "   Best CV Score: 0.9269\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      ccp_alpha: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      criterion: friedman_mse (ğŸ“‹ DEFAULT)\n",
      "      init: None (ğŸ“‹ DEFAULT)\n",
      "      learning_rate: 0.1 (ğŸ¯ TUNED)\n",
      "      loss: log_loss (ğŸ“‹ DEFAULT)\n",
      "      max_depth: 3 (ğŸ¯ TUNED)\n",
      "      max_features: None (ğŸ“‹ DEFAULT)\n",
      "      max_leaf_nodes: None (ğŸ“‹ DEFAULT)\n",
      "      min_impurity_decrease: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      min_samples_leaf: 1 (ğŸ“‹ DEFAULT)\n",
      "      min_samples_split: 2 (ğŸ“‹ DEFAULT)\n",
      "      min_weight_fraction_leaf: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      n_estimators: 100 (ğŸ¯ TUNED)\n",
      "      n_iter_no_change: None (ğŸ“‹ DEFAULT)\n",
      "      random_state: 42 (âš¡ REPRO)\n",
      "      subsample: 1.0 (ğŸ“‹ DEFAULT)\n",
      "      tol: 0.0001 (ğŸ“‹ DEFAULT)\n",
      "      validation_fraction: 0.1 (ğŸ“‹ DEFAULT)\n",
      "      verbose: 0 (ğŸ“‹ DEFAULT)\n",
      "      warm_start: False (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ AdaBoostClassifier:\n",
      "   Best CV Score: 0.8441\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      algorithm: deprecated (ğŸ“‹ DEFAULT)\n",
      "      estimator: None (ğŸ“‹ DEFAULT)\n",
      "      learning_rate: 1.0 (ğŸ¯ TUNED)\n",
      "      n_estimators: 100 (ğŸ¯ TUNED)\n",
      "      random_state: 42 (âš¡ REPRO)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ XGBClassifier:\n",
      "   Best CV Score: 0.9302\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      base_score: None (ğŸ“‹ DEFAULT)\n",
      "      booster: None (ğŸ“‹ DEFAULT)\n",
      "      callbacks: None (ğŸ“‹ DEFAULT)\n",
      "      colsample_bylevel: None (ğŸ“‹ DEFAULT)\n",
      "      colsample_bynode: None (ğŸ“‹ DEFAULT)\n",
      "      colsample_bytree: None (ğŸ“‹ DEFAULT)\n",
      "      device: None (ğŸ“‹ DEFAULT)\n",
      "      early_stopping_rounds: None (ğŸ“‹ DEFAULT)\n",
      "      enable_categorical: False (ğŸ“‹ DEFAULT)\n",
      "      eval_metric: None (ğŸ“‹ DEFAULT)\n",
      "      feature_types: None (ğŸ“‹ DEFAULT)\n",
      "      feature_weights: None (ğŸ“‹ DEFAULT)\n",
      "      gamma: None (ğŸ“‹ DEFAULT)\n",
      "      grow_policy: None (ğŸ“‹ DEFAULT)\n",
      "      importance_type: None (ğŸ“‹ DEFAULT)\n",
      "      interaction_constraints: None (ğŸ“‹ DEFAULT)\n",
      "      learning_rate: 0.2 (ğŸ¯ TUNED)\n",
      "      max_bin: None (ğŸ“‹ DEFAULT)\n",
      "      max_cat_threshold: None (ğŸ“‹ DEFAULT)\n",
      "      max_cat_to_onehot: None (ğŸ“‹ DEFAULT)\n",
      "      max_delta_step: None (ğŸ“‹ DEFAULT)\n",
      "      max_depth: 6 (ğŸ¯ TUNED)\n",
      "      max_leaves: None (ğŸ“‹ DEFAULT)\n",
      "      min_child_weight: None (ğŸ“‹ DEFAULT)\n",
      "      missing: nan (ğŸ“‹ DEFAULT)\n",
      "      monotone_constraints: None (ğŸ“‹ DEFAULT)\n",
      "      multi_strategy: None (ğŸ“‹ DEFAULT)\n",
      "      n_estimators: 100 (ğŸ¯ TUNED)\n",
      "      n_jobs: -1 (âš¡ REPRO)\n",
      "      num_parallel_tree: None (ğŸ“‹ DEFAULT)\n",
      "      objective: binary:logistic (ğŸ“‹ DEFAULT)\n",
      "      random_state: 42 (âš¡ REPRO)\n",
      "      reg_alpha: None (ğŸ“‹ DEFAULT)\n",
      "      reg_lambda: None (ğŸ“‹ DEFAULT)\n",
      "      sampling_method: None (ğŸ“‹ DEFAULT)\n",
      "      scale_pos_weight: None (ğŸ“‹ DEFAULT)\n",
      "      subsample: 1.0 (ğŸ¯ TUNED)\n",
      "      tree_method: None (ğŸ“‹ DEFAULT)\n",
      "      validate_parameters: None (ğŸ“‹ DEFAULT)\n",
      "      verbosity: None (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ LogisticRegression:\n",
      "   Best CV Score: 0.9243\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      C: 10 (ğŸ¯ TUNED)\n",
      "      class_weight: None (ğŸ“‹ DEFAULT)\n",
      "      dual: False (ğŸ“‹ DEFAULT)\n",
      "      fit_intercept: True (ğŸ“‹ DEFAULT)\n",
      "      intercept_scaling: 1 (ğŸ“‹ DEFAULT)\n",
      "      l1_ratio: None (ğŸ“‹ DEFAULT)\n",
      "      max_iter: 100 (ğŸ“‹ DEFAULT)\n",
      "      multi_class: deprecated (ğŸ“‹ DEFAULT)\n",
      "      n_jobs: -1 (âš¡ REPRO)\n",
      "      penalty: l2 (ğŸ¯ TUNED)\n",
      "      random_state: 42 (âš¡ REPRO)\n",
      "      solver: lbfgs (ğŸ¯ TUNED)\n",
      "      tol: 0.0001 (ğŸ“‹ DEFAULT)\n",
      "      verbose: 0 (ğŸ“‹ DEFAULT)\n",
      "      warm_start: False (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ RidgeClassifier:\n",
      "   Best CV Score: 0.9026\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      alpha: 1.0 (ğŸ¯ TUNED)\n",
      "      class_weight: None (ğŸ“‹ DEFAULT)\n",
      "      copy_X: True (ğŸ“‹ DEFAULT)\n",
      "      fit_intercept: True (ğŸ“‹ DEFAULT)\n",
      "      max_iter: None (ğŸ“‹ DEFAULT)\n",
      "      positive: False (ğŸ“‹ DEFAULT)\n",
      "      random_state: 42 (âš¡ REPRO)\n",
      "      solver: auto (ğŸ¯ TUNED)\n",
      "      tol: 0.0001 (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ SGDClassifier:\n",
      "   Best CV Score: 0.919\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      alpha: 0.0001 (ğŸ¯ TUNED)\n",
      "      average: False (ğŸ“‹ DEFAULT)\n",
      "      class_weight: None (ğŸ“‹ DEFAULT)\n",
      "      early_stopping: False (ğŸ“‹ DEFAULT)\n",
      "      epsilon: 0.1 (ğŸ“‹ DEFAULT)\n",
      "      eta0: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      fit_intercept: True (ğŸ“‹ DEFAULT)\n",
      "      l1_ratio: 0.15 (ğŸ“‹ DEFAULT)\n",
      "      learning_rate: optimal (ğŸ“‹ DEFAULT)\n",
      "      loss: hinge (ğŸ¯ TUNED)\n",
      "      max_iter: 1000 (ğŸ“‹ DEFAULT)\n",
      "      n_iter_no_change: 5 (ğŸ“‹ DEFAULT)\n",
      "      n_jobs: -1 (âš¡ REPRO)\n",
      "      penalty: l1 (ğŸ¯ TUNED)\n",
      "      power_t: 0.5 (ğŸ“‹ DEFAULT)\n",
      "      random_state: 42 (âš¡ REPRO)\n",
      "      shuffle: True (ğŸ“‹ DEFAULT)\n",
      "      tol: 0.001 (ğŸ“‹ DEFAULT)\n",
      "      validation_fraction: 0.1 (ğŸ“‹ DEFAULT)\n",
      "      verbose: 0 (ğŸ“‹ DEFAULT)\n",
      "      warm_start: False (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ Perceptron:\n",
      "   Best CV Score: 0.9021\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      alpha: 0.0001 (ğŸ¯ TUNED)\n",
      "      class_weight: None (ğŸ“‹ DEFAULT)\n",
      "      early_stopping: False (ğŸ“‹ DEFAULT)\n",
      "      eta0: 1.0 (ğŸ“‹ DEFAULT)\n",
      "      fit_intercept: True (ğŸ“‹ DEFAULT)\n",
      "      l1_ratio: 0.15 (ğŸ“‹ DEFAULT)\n",
      "      max_iter: 1000 (ğŸ“‹ DEFAULT)\n",
      "      n_iter_no_change: 5 (ğŸ“‹ DEFAULT)\n",
      "      n_jobs: -1 (âš¡ REPRO)\n",
      "      penalty: None (ğŸ¯ TUNED)\n",
      "      random_state: 42 (âš¡ REPRO)\n",
      "      shuffle: True (ğŸ“‹ DEFAULT)\n",
      "      tol: 0.001 (ğŸ“‹ DEFAULT)\n",
      "      validation_fraction: 0.1 (ğŸ“‹ DEFAULT)\n",
      "      verbose: 0 (ğŸ“‹ DEFAULT)\n",
      "      warm_start: False (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ KNeighborsClassifier:\n",
      "   Best CV Score: 0.925\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      algorithm: auto (ğŸ“‹ DEFAULT)\n",
      "      leaf_size: 30 (ğŸ“‹ DEFAULT)\n",
      "      metric: euclidean (ğŸ¯ TUNED)\n",
      "      metric_params: None (ğŸ“‹ DEFAULT)\n",
      "      n_jobs: -1 (âš¡ REPRO)\n",
      "      n_neighbors: 7 (ğŸ¯ TUNED)\n",
      "      p: 2 (ğŸ“‹ DEFAULT)\n",
      "      weights: distance (ğŸ¯ TUNED)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ SVC:\n",
      "   Best CV Score: 0.9314\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      C: 10 (ğŸ¯ TUNED)\n",
      "      break_ties: False (ğŸ“‹ DEFAULT)\n",
      "      cache_size: 200 (ğŸ“‹ DEFAULT)\n",
      "      class_weight: None (ğŸ“‹ DEFAULT)\n",
      "      coef0: 0.0 (ğŸ“‹ DEFAULT)\n",
      "      decision_function_shape: ovr (ğŸ“‹ DEFAULT)\n",
      "      degree: 3 (ğŸ“‹ DEFAULT)\n",
      "      gamma: scale (ğŸ¯ TUNED)\n",
      "      kernel: rbf (ğŸ¯ TUNED)\n",
      "      max_iter: -1 (ğŸ“‹ DEFAULT)\n",
      "      probability: False (ğŸ“‹ DEFAULT)\n",
      "      random_state: 42 (âš¡ REPRO)\n",
      "      shrinking: True (ğŸ“‹ DEFAULT)\n",
      "      tol: 0.001 (ğŸ“‹ DEFAULT)\n",
      "      verbose: False (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ GaussianNB:\n",
      "   Best CV Score: 0.8974\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      priors: None (ğŸ“‹ DEFAULT)\n",
      "      var_smoothing: 1e-09 (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ LinearDiscriminantAnalysis:\n",
      "   Best CV Score: 0.9046\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      covariance_estimator: None (ğŸ“‹ DEFAULT)\n",
      "      n_components: None (ğŸ“‹ DEFAULT)\n",
      "      priors: None (ğŸ“‹ DEFAULT)\n",
      "      shrinkage: None (ğŸ¯ TUNED)\n",
      "      solver: svd (ğŸ¯ TUNED)\n",
      "      store_covariance: False (ğŸ“‹ DEFAULT)\n",
      "      tol: 0.0001 (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ QuadraticDiscriminantAnalysis:\n",
      "   Best CV Score: 0.9194\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      priors: None (ğŸ“‹ DEFAULT)\n",
      "      reg_param: 0.1 (ğŸ¯ TUNED)\n",
      "      store_covariance: False (ğŸ“‹ DEFAULT)\n",
      "      tol: 0.0001 (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”§ MLPClassifier:\n",
      "   Best CV Score: 0.9338\n",
      "   Search Method: grid\n",
      "   Scoring Metric: accuracy\n",
      "   All Parameters:\n",
      "      activation: relu (ğŸ¯ TUNED)\n",
      "      alpha: 0.01 (ğŸ¯ TUNED)\n",
      "      batch_size: auto (ğŸ“‹ DEFAULT)\n",
      "      beta_1: 0.9 (ğŸ“‹ DEFAULT)\n",
      "      beta_2: 0.999 (ğŸ“‹ DEFAULT)\n",
      "      early_stopping: False (ğŸ“‹ DEFAULT)\n",
      "      epsilon: 1e-08 (ğŸ“‹ DEFAULT)\n",
      "      hidden_layer_sizes: (50,) (ğŸ¯ TUNED)\n",
      "      learning_rate: constant (ğŸ“‹ DEFAULT)\n",
      "      learning_rate_init: 0.001 (ğŸ“‹ DEFAULT)\n",
      "      max_fun: 15000 (ğŸ“‹ DEFAULT)\n",
      "      max_iter: 200 (ğŸ“‹ DEFAULT)\n",
      "      momentum: 0.9 (ğŸ“‹ DEFAULT)\n",
      "      n_iter_no_change: 10 (ğŸ“‹ DEFAULT)\n",
      "      nesterovs_momentum: True (ğŸ“‹ DEFAULT)\n",
      "      power_t: 0.5 (ğŸ“‹ DEFAULT)\n",
      "      random_state: 42 (âš¡ REPRO)\n",
      "      shuffle: True (ğŸ“‹ DEFAULT)\n",
      "      solver: adam (ğŸ¯ TUNED)\n",
      "      tol: 0.0001 (ğŸ“‹ DEFAULT)\n",
      "      validation_fraction: 0.1 (ğŸ“‹ DEFAULT)\n",
      "      verbose: False (ğŸ“‹ DEFAULT)\n",
      "      warm_start: False (ğŸ“‹ DEFAULT)\n",
      "----------------------------------------\n",
      "\n",
      "Summary: Tuned parameters for 16 models\n",
      "Detailed results saved to: C:\\Misc\\ml_benchmark\\outputs\\tuned_models\\best_params.json\n",
      "\n",
      "âš ï¸  Models with no hyperparameter grid (used defaults only):\n",
      "   â€¢ GaussianNB\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 6. CONFIRM TUNED AND REPRODUCIBILITY PARAMETERS USED IN EACH MODEL\n",
    "# -------------------------------------------\n",
    "\n",
    "# This code cell shows the actual tuned parameters that were determined during GridSearchCV.\n",
    "# This code cell also confirms whether the default parameters that are defined for some models in the \n",
    "# AVAILABLE_MODELS dictionary (random_state=42, n_jobs=-1) were injected into the best_params list.\n",
    "# These last two parameters support reproducibility and parallel processing, respectively.\n",
    "# The display provides an audit trail of what parameter combinations actually were selected as \"best\"\n",
    "# for each model, including either of the two injected reproducibility parameters.\n",
    "# All other sklearn default parameters that each model used internally are not shown here.\n",
    "\n",
    "print(\"FINAL TUNED PARAMETERS FOR EACH MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, summary in best_params_summary.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"   Best CV Score: {summary['best_score']}\")\n",
    "    print(f\"   Search Method: {summary['search_type']}\")\n",
    "    print(f\"   Scoring Metric: {summary['scoring']}\")\n",
    "    print(\"   All Parameters:\")\n",
    "    \n",
    "    # Get the model configuration\n",
    "    config = AVAILABLE_MODELS[model_name]\n",
    "    best_params = summary['best_params']\n",
    "    \n",
    "    # Create a model instance to get all parameters\n",
    "    module = importlib.import_module(config[\"module\"])\n",
    "    model_class = getattr(module, config[\"class\"])\n",
    "    model_instance = model_class(**best_params)\n",
    "    all_params = model_instance.get_params()\n",
    "    \n",
    "    # Get lists of parameter types for labeling\n",
    "    param_grid_keys = set(config.get(\"param_grid\", {}).keys())\n",
    "    reproducibility_params = {'random_state', 'n_jobs'}\n",
    "    \n",
    "    # Sort parameters alphabetically for consistent display\n",
    "    for param_name in sorted(all_params.keys()):\n",
    "        param_value = all_params[param_name]\n",
    "        \n",
    "        # Determine parameter type and add appropriate label\n",
    "        labels = []\n",
    "        if param_name in param_grid_keys:\n",
    "            labels.append(\"ğŸ¯ TUNED\")\n",
    "        if param_name in reproducibility_params:\n",
    "            labels.append(\"âš¡ REPRO\")\n",
    "        if not labels:\n",
    "            labels.append(\"ğŸ“‹ DEFAULT\")\n",
    "        \n",
    "        label_str = \" \".join(labels)\n",
    "        print(f\"      {param_name}: {param_value} ({label_str})\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\nSummary: Tuned parameters for {len(best_params_summary)} models\")\n",
    "print(f\"Detailed results saved to: {tuned_models_dir / 'best_params.json'}\")\n",
    "\n",
    "# Optional: Show models that had no hyperparameters to tune\n",
    "no_tuning = [name for name, summary in best_params_summary.items() \n",
    "             if not AVAILABLE_MODELS[name]['param_grid']]\n",
    "\n",
    "if no_tuning:\n",
    "    print(f\"\\nâš ï¸  Models with no hyperparameter grid (used defaults only):\")\n",
    "    for model in no_tuning:\n",
    "        print(f\"   â€¢ {model}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0e7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table of Tuned Hyperparameters and Search Spaces:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Hyperparameter (search space)</th>\n",
       "      <th>Tuned Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>learning_rate (choices: 0.5, 1.0)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>n_estimators (choices: 50, 100)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>criterion (choices: gini, entropy)</td>\n",
       "      <td>gini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>max_depth (choices: None, 5, 10, 20)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>min_samples_leaf (choices: 1, 3)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>min_samples_split (choices: 2, 5)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>max_depth (choices: None, 10, 20)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>min_samples_split (choices: 2, 5)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>n_estimators (choices: 50, 100)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>(no hyperparameters tuned)</td>\n",
       "      <td>(sklearn defaults)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>learning_rate (choices: 0.05, 0.1)</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>max_depth (choices: 3, 5)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>n_estimators (choices: 50, 100)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>metric (choices: euclidean, manhattan)</td>\n",
       "      <td>euclidean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>n_neighbors (choices: 3, 5, 7)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>weights (choices: uniform, distance)</td>\n",
       "      <td>distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LinearDiscriminantAnalysis</td>\n",
       "      <td>shrinkage (choices: None, auto)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LinearDiscriminantAnalysis</td>\n",
       "      <td>solver (choices: svd, lsqr)</td>\n",
       "      <td>svd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>C (choices: 0.1, 1, 10)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>penalty (choices: l2)</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>solver (choices: lbfgs, liblinear)</td>\n",
       "      <td>lbfgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>activation (choices: relu, tanh)</td>\n",
       "      <td>relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>alpha (choices: 0.0001, 0.001, 0.01)</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>hidden_layer_sizes (choices: (50,), (100,), (5...</td>\n",
       "      <td>(50,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>solver (choices: adam, lbfgs)</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>alpha (choices: 0.0001, 0.001)</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>penalty (choices: l2, elasticnet, None)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>reg_param (choices: 0.0, 0.1, 0.5)</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>max_depth (choices: None, 10, 20)</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>min_samples_leaf (choices: 1, 3)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>min_samples_split (choices: 2, 5)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>n_estimators (choices: 50, 100)</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>alpha (choices: 0.1, 1.0, 10.0)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>solver (choices: auto, sparse_cg)</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>alpha (choices: 0.0001, 0.001)</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>loss (choices: hinge, log_loss)</td>\n",
       "      <td>hinge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>penalty (choices: l2, l1)</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SVC</td>\n",
       "      <td>C (choices: 0.1, 1, 10)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SVC</td>\n",
       "      <td>gamma (choices: scale, auto)</td>\n",
       "      <td>scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>SVC</td>\n",
       "      <td>kernel (choices: linear, rbf)</td>\n",
       "      <td>rbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>learning_rate (choices: 0.1, 0.2)</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>max_depth (choices: 3, 6)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>n_estimators (choices: 50, 100)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>subsample (choices: 0.8, 1.0)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model  \\\n",
       "15             AdaBoostClassifier   \n",
       "14             AdaBoostClassifier   \n",
       "0          DecisionTreeClassifier   \n",
       "1          DecisionTreeClassifier   \n",
       "3          DecisionTreeClassifier   \n",
       "2          DecisionTreeClassifier   \n",
       "9            ExtraTreesClassifier   \n",
       "10           ExtraTreesClassifier   \n",
       "8            ExtraTreesClassifier   \n",
       "36                     GaussianNB   \n",
       "12     GradientBoostingClassifier   \n",
       "13     GradientBoostingClassifier   \n",
       "11     GradientBoostingClassifier   \n",
       "32           KNeighborsClassifier   \n",
       "30           KNeighborsClassifier   \n",
       "31           KNeighborsClassifier   \n",
       "38     LinearDiscriminantAnalysis   \n",
       "37     LinearDiscriminantAnalysis   \n",
       "20             LogisticRegression   \n",
       "21             LogisticRegression   \n",
       "22             LogisticRegression   \n",
       "41                  MLPClassifier   \n",
       "43                  MLPClassifier   \n",
       "40                  MLPClassifier   \n",
       "42                  MLPClassifier   \n",
       "29                     Perceptron   \n",
       "28                     Perceptron   \n",
       "39  QuadraticDiscriminantAnalysis   \n",
       "5          RandomForestClassifier   \n",
       "7          RandomForestClassifier   \n",
       "6          RandomForestClassifier   \n",
       "4          RandomForestClassifier   \n",
       "23                RidgeClassifier   \n",
       "24                RidgeClassifier   \n",
       "26                  SGDClassifier   \n",
       "25                  SGDClassifier   \n",
       "27                  SGDClassifier   \n",
       "33                            SVC   \n",
       "35                            SVC   \n",
       "34                            SVC   \n",
       "18                  XGBClassifier   \n",
       "17                  XGBClassifier   \n",
       "16                  XGBClassifier   \n",
       "19                  XGBClassifier   \n",
       "\n",
       "                        Hyperparameter (search space)         Tuned Value  \n",
       "15                  learning_rate (choices: 0.5, 1.0)                 1.0  \n",
       "14                    n_estimators (choices: 50, 100)                 100  \n",
       "0                  criterion (choices: gini, entropy)                gini  \n",
       "1                max_depth (choices: None, 5, 10, 20)                  10  \n",
       "3                    min_samples_leaf (choices: 1, 3)                   1  \n",
       "2                   min_samples_split (choices: 2, 5)                   5  \n",
       "9                   max_depth (choices: None, 10, 20)                None  \n",
       "10                  min_samples_split (choices: 2, 5)                   5  \n",
       "8                     n_estimators (choices: 50, 100)                 100  \n",
       "36                         (no hyperparameters tuned)  (sklearn defaults)  \n",
       "12                 learning_rate (choices: 0.05, 0.1)                 0.1  \n",
       "13                          max_depth (choices: 3, 5)                   3  \n",
       "11                    n_estimators (choices: 50, 100)                 100  \n",
       "32             metric (choices: euclidean, manhattan)           euclidean  \n",
       "30                     n_neighbors (choices: 3, 5, 7)                   7  \n",
       "31               weights (choices: uniform, distance)            distance  \n",
       "38                    shrinkage (choices: None, auto)                None  \n",
       "37                        solver (choices: svd, lsqr)                 svd  \n",
       "20                            C (choices: 0.1, 1, 10)                  10  \n",
       "21                              penalty (choices: l2)                  l2  \n",
       "22                 solver (choices: lbfgs, liblinear)               lbfgs  \n",
       "41                   activation (choices: relu, tanh)                relu  \n",
       "43               alpha (choices: 0.0001, 0.001, 0.01)                0.01  \n",
       "40  hidden_layer_sizes (choices: (50,), (100,), (5...               (50,)  \n",
       "42                      solver (choices: adam, lbfgs)                adam  \n",
       "29                     alpha (choices: 0.0001, 0.001)              0.0001  \n",
       "28            penalty (choices: l2, elasticnet, None)                None  \n",
       "39                 reg_param (choices: 0.0, 0.1, 0.5)                 0.1  \n",
       "5                   max_depth (choices: None, 10, 20)                  20  \n",
       "7                    min_samples_leaf (choices: 1, 3)                   1  \n",
       "6                   min_samples_split (choices: 2, 5)                   2  \n",
       "4                     n_estimators (choices: 50, 100)                  50  \n",
       "23                    alpha (choices: 0.1, 1.0, 10.0)                 1.0  \n",
       "24                  solver (choices: auto, sparse_cg)                auto  \n",
       "26                     alpha (choices: 0.0001, 0.001)              0.0001  \n",
       "25                    loss (choices: hinge, log_loss)               hinge  \n",
       "27                          penalty (choices: l2, l1)                  l1  \n",
       "33                            C (choices: 0.1, 1, 10)                  10  \n",
       "35                       gamma (choices: scale, auto)               scale  \n",
       "34                      kernel (choices: linear, rbf)                 rbf  \n",
       "18                  learning_rate (choices: 0.1, 0.2)                 0.2  \n",
       "17                          max_depth (choices: 3, 6)                   6  \n",
       "16                    n_estimators (choices: 50, 100)                 100  \n",
       "19                      subsample (choices: 0.8, 1.0)                 1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table saved to: C:\\Misc\\ml_benchmark\\outputs\\tuned_models\\tuned_hyperparameters_table.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 7. PRODUCE TABLE OF HYPERPARAMETER CHOICES AND FINAL TUNED VALUES\n",
    "# -------------------------------------------\n",
    "\n",
    "# The table produced here is used in  this benchmarking project's final report to \n",
    "# summarize hyperparameter tuning results and to support explanations of\n",
    "# a model's accuracy and runtime results.\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "rows = []\n",
    "\n",
    "for model_name, summary in best_params_summary.items():\n",
    "    config = AVAILABLE_MODELS[model_name]\n",
    "    param_grid = config.get('param_grid', {})\n",
    "    best_params = summary['best_params']\n",
    "    \n",
    "    if not param_grid:\n",
    "        # No hyperparameters tuned for this model\n",
    "        rows.append({\n",
    "            'Model': model_name,\n",
    "            'Hyperparameter (search space)': '(no hyperparameters tuned)',\n",
    "            'Tuned Value': '(sklearn defaults)'\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    for param, choices in param_grid.items():\n",
    "        tuned_value = best_params.get(param, '(not set)')\n",
    "        # Format choices for display\n",
    "        if isinstance(choices, list):\n",
    "            choices_str = ', '.join([str(c) for c in choices])\n",
    "        else:\n",
    "            choices_str = str(choices)\n",
    "        rows.append({\n",
    "            'Model': model_name,\n",
    "            'Hyperparameter (search space)': f\"{param} (choices: {choices_str})\",\n",
    "            'Tuned Value': tuned_value\n",
    "        })\n",
    "\n",
    "# Create DataFrame for display\n",
    "param_table = pd.DataFrame(rows)\n",
    "\n",
    "# Optionally, sort by model name\n",
    "param_table = param_table.sort_values(['Model', 'Hyperparameter (search space)'])\n",
    "\n",
    "# Display the table\n",
    "print(\"\\nTable of Tuned Hyperparameters and Search Spaces:\")\n",
    "display(param_table)\n",
    "\n",
    "# Optionally, save to Excel for inclusion in report\n",
    "param_table_path = tuned_models_dir / \"tuned_hyperparameters_table.xlsx\"\n",
    "param_table.to_excel(param_table_path, index=False)\n",
    "print(f\"\\nTable saved to: {param_table_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
