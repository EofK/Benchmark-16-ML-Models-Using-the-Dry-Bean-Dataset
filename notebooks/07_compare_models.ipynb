{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b0bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 16 tuned models from Stage 06\n",
      "üìÇ Data source: C:\\Misc\\ml_benchmark\\outputs\\curated_data\\DryBean_curated.parquet\n",
      "üìÇ Results will be saved to: C:\\Misc\\ml_benchmark\\outputs\\results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------\n",
    "# 1. IMPORT, DEFINE PATHS, AND LOAD TUNED MODELS\n",
    "# -------------------------------------------\n",
    "\n",
    "# Stage 07: Accuracy and runtime calculations for a selected model\n",
    "#\n",
    "\n",
    "# The user selects a single model to evaluate accuracy and runtime.\n",
    "# This module then loads the selected tuned model's dictionary from Stage 06 and:\n",
    "#   1. Performs exhaustive feature subset analysis to measure an average runtime for the model\n",
    "#   2. Generates a confusion matrix and detailed accuracy report for the model.\n",
    "#   3. Saves results to three Excel files for just the selected model:\n",
    "#        a. A main results Excel file showing accuracy and runtime for each feature combination\n",
    "#           (this Excel file is used only for its runtime results).\n",
    "#        b. A per-class accuracy summary file showing how well the model predicted each bean variety.\n",
    "#        c. A validation report file with the model's confusion matrix and detailed accuracy report.\n",
    "#    4. Adds the model's confusion matrix to an existing Excel file containing all models' confusion matrices.\n",
    "# \n",
    "\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define this project's file locations.\n",
    "# This notebook uses a centralized config.py file for all path management.\n",
    "\n",
    "# Import config paths\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import TUNED_MODELS_DIR, CURATED_DATA_DIR, RESULTS_DIR\n",
    "\n",
    "curated_data_dir = CURATED_DATA_DIR\n",
    "results_dir = RESULTS_DIR\n",
    "tuned_models_dir = TUNED_MODELS_DIR\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "data_path = CURATED_DATA_DIR / \"DryBean_curated.parquet\"    # From Stage 04\n",
    "models_path = TUNED_MODELS_DIR / \"model_dict.joblib\"        # From Stage 06\n",
    "\n",
    "# Load tuned model dictionary from Stage 06\n",
    "model_dict = joblib.load(models_path)\n",
    "print(f\"Loaded {len(model_dict)} tuned models from Stage 06\")\n",
    "print(f\"Data source: {data_path}\")\n",
    "print(f\"Results will be saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d245ba6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Available tuned models, listed in ML algorithm family order:\n",
      "    1. DecisionTreeClassifier\n",
      "    2. RandomForestClassifier\n",
      "    3. ExtraTreesClassifier\n",
      "    4. GradientBoostingClassifier\n",
      "    5. AdaBoostClassifier\n",
      "    6. XGBClassifier\n",
      "    7. LogisticRegression\n",
      "    8. RidgeClassifier\n",
      "    9. SGDClassifier\n",
      "   10. Perceptron\n",
      "   11. SVC\n",
      "   12. KNeighborsClassifier\n",
      "   13. GaussianNB\n",
      "   14. LinearDiscriminantAnalysis\n",
      "   15. QuadraticDiscriminantAnalysis\n",
      "   16. MLPClassifier\n",
      "\n",
      "‚úÖ Selected model: AdaBoostClassifier\n",
      "üîç Testing feature set combinations from 14 to all features\n",
      "Model parameters:\n",
      "   algorithm=deprecated, estimator=None, learning_rate=1.0, n_estimators=100, random_state=42\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 2. SELECT MODEL AND FEATURE SETS\n",
    "# -------------------------------------------\n",
    "\n",
    "# Import json for loading two required dictionaries saved from Stage 05\n",
    "import json\n",
    "\n",
    "\n",
    "# Display available ML models in the order set in MODEL_FAMILIES dictionary\n",
    "with open(tuned_models_dir / \"model_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    MODEL_FAMILIES = config['MODEL_FAMILIES']\n",
    "    model_mapping = config['model_mapping']\n",
    "    \n",
    "ordered_models = [model_mapping[short] for family in MODEL_FAMILIES.values() for _, short in family if model_mapping[short] in model_dict]\n",
    "print(\"\\n Available tuned models, listed in ML algorithm family order:\")\n",
    "for i, model in enumerate(ordered_models, 1):\n",
    "    print(f\"   {i:2d}. {model}\")\n",
    "\n",
    "\n",
    "# ------------- SELECT MODEL TO TEST HERE -------------\n",
    "# Change this variable to select the model to test\n",
    "selected_model_name = \"AdaBoostClassifier\"  # Change this to test different models\n",
    "# -----------------------------------------------------\n",
    "\n",
    "if selected_model_name not in model_dict:\n",
    "    print(f\"‚ùå Error: {selected_model_name} not found in model_dict\")\n",
    "    print(f\"Available models: {available_models}\")\n",
    "else:\n",
    "    model = model_dict[selected_model_name]\n",
    "    print(f\"\\n‚úÖ Selected model: {selected_model_name}\")\n",
    "    \n",
    "    # Feature Selection Parameters\n",
    "    # ------ ENTER THE MINIMUM FEATURE COUNT HERE -------\n",
    "    min_features = 14 \n",
    "    print(f\"üîç Testing feature set combinations from {min_features} to all features\")\n",
    "    # ---------------------------------------------------\n",
    "\n",
    "    # Display the model's parameters\n",
    "    params = model.get_params()\n",
    "    param_items = list(params.items())\n",
    "    print(f\"Model parameters:\")\n",
    "    for i in range(0, len(param_items), 5):\n",
    "        chunk = param_items[i:i+5]\n",
    "        param_str = \", \".join([f\"{k}={v}\" for k, v in chunk])\n",
    "        print(f\"   {param_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbade755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset Information:\n",
      "   Shape: (13611, 16)\n",
      "   Total features: 16\n",
      "   Feature names: ['A', 'P', 'L', 'l', 'K', 'Ec', 'C', 'Ed', 'Ex', 'S', 'R', 'CO', 'SF1', 'SF2', 'SF3', 'SF4']\n",
      "   Target classes: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "   Data types: Features=float64, Target=int64\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 3. LOAD AND PREPROCESS DATA\n",
    "# -------------------------------------------\n",
    "\n",
    "# Load processed dataset from Stage 04 (scaled and encoded)\n",
    "df = pd.read_parquet(data_path)\n",
    "X = df.drop(columns=[\"label\"])  # Features (already scaled/encoded from Stage 04)\n",
    "y = df[\"label\"]                 # Target (already label-encoded from Stage 04)\n",
    "\n",
    "# The data was preprocessed in Stage 04, so can be used here\n",
    "# without additional scaling or encoding.\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "total_features = len(feature_names)\n",
    "unique_classes = y.unique()\n",
    "\n",
    "# Confirm key dataset information. The last print line confirmas that: \n",
    "#   1. Target classes are numeric (some models require numeric labels), and\n",
    "#   2. No classes are missing.\n",
    "print(f\"\\n Dataset Information:\")\n",
    "print(f\"   Shape: {X.shape}\")\n",
    "print(f\"   Total features: {total_features}\")\n",
    "print(f\"   Feature names: {feature_names}\")\n",
    "print(f\"   Target classes: {sorted(unique_classes)}\")\n",
    "print(f\"   Data types: Features={X.dtypes.iloc[0]}, Target={y.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90602647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bean varieties to track: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "\n",
      "üîÑ Starting exhaustive feature subset analysis...\n",
      "   Total combinations to test: 137\n",
      "   Feature range: 14 to 16 features\n",
      "\n",
      "üß™ Testing 120 combinations of 14 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature combinations (14 features): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120/120 [47:07<00:00, 23.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing 16 combinations of 15 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature combinations (15 features): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [06:39<00:00, 24.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing 1 combinations of 16 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature combinations (16 features): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:26<00:00, 26.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Completed testing 137 feature combinations\n",
      "Per-class data collected for 137 feature combinations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 4. MEASURE AND RECORD A MODEL'S PERFORMANCE \n",
    "# -------------------------------------------\n",
    "\n",
    "# This code cell is the source of each model's runtimes used in the final results.\n",
    "\n",
    "# This code creates multiple runs on a range of dataset feature subsets to smooth out runtime \n",
    "# variability caused by Windows background activity (e.g., system services or scheduled \n",
    "# tasks temporarily affecting CPU availability and RAM management). \n",
    "\n",
    "\n",
    "# This code cell creates, for each dataset record, all possible feature sets \n",
    "# containing 14 features (120), 15 features (16) , and the original 16-feature record (1) = 137 feature sets.\n",
    "# This code then loops through tests of the selected model \n",
    "# on every feature set created for every record.  Each feature set is examined the number of times \n",
    "# set by the user in the variable num_runs. Meaning, each feature set can be examined 1, 2, 3,... times.\n",
    "\n",
    "# The average accuracy and runtime across all three loops of the same feature set are recorded.\n",
    "# NOTE: Only the runtime results from this code cell are used in the final results. Accuracy results \n",
    "# used for the project's report are determined in a later code cell, below.\n",
    "\n",
    "# Additionally, the code cell measures how well the model predicts each of the 7 bean varieties.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "per_class_data = []  # Each model's accuracy in predicing each bean variety (aka, class) is stored here.\n",
    "\n",
    "# Three loops taken on each dataset record. Used to possibly offset runtime outliers caused\n",
    "# by Windows background processes and RAM management interfering with the model's runtime.\n",
    "num_runs = 3  \n",
    "\n",
    "# Get unique class names for consistent reporting\n",
    "class_names = sorted(y.unique())\n",
    "print(f\"Bean varieties to track: {class_names}\")\n",
    "\n",
    "# Calculate expected combinations for progress tracking.\n",
    "total_combinations = sum(len(list(itertools.combinations(feature_names, r))) \n",
    "                         for r in range(min_features, total_features + 1))\n",
    "print(f\"\\nStarting exhaustive feature subset analysis...\")\n",
    "print(f\"   Total combinations to test: {total_combinations:,}\")\n",
    "print(f\"   Feature range: {min_features} to {total_features} features\")\n",
    "\n",
    "# Conduct benchmark: repeat each feature set 3x, then determine average accuracy and runtime\n",
    "for r in range(min_features, len(feature_names) + 1):\n",
    "    combos = list(itertools.combinations(feature_names, r))\n",
    "    print(f\"\\nTesting {len(combos):,} combinations of {r} features...\")\n",
    "\n",
    "    for combo in tqdm(combos, desc=f\"Feature combinations ({r} features)\"):\n",
    "        accs = []\n",
    "        runtimes = []\n",
    "        \n",
    "        # Track per-class accuracy across all runs\n",
    "        class_accuracy_runs = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            model = model_dict[selected_model_name]  # fresh model each run\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "            start_time = time.perf_counter()   #----------------START CLOCK FOR RUNIME CALCULATION-----------------\n",
    "            \n",
    "            # Collect predictions for per-class analysis\n",
    "            y_true_all = []\n",
    "            y_pred_all = []\n",
    "            \n",
    "            # Manual cross-validation to capture individual predictions - generally same runtime for all models\n",
    "            for train_idx, test_idx in cv.split(X[list(combo)], y):\n",
    "                X_train, X_test = X[list(combo)].iloc[train_idx], X[list(combo)].iloc[test_idx]\n",
    "                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "                \n",
    "                \n",
    "                # Train model on 80% of dataset - COMPLETELY DIFFERENT for each model, so different runtimes\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Generate predictions on 20% of dataset - COMPLETELY DIFFERENT for each model, so different\n",
    "                # runtimes and accuracies\n",
    "                y_pred = model.predict(X_test)   \n",
    "                \n",
    "                # Store for per-class analysis - generally same runtime for all models\n",
    "                y_true_all.extend(y_test.tolist())\n",
    "                y_pred_all.extend(y_pred.tolist())\n",
    "            \n",
    "            end_time = time.perf_counter()      #----------------STOP CLOCK FOR RUNIME CALCULATION-----------------\n",
    "            \n",
    "            # Calculate overall accuracy\n",
    "            overall_accuracy = sum(yt == yp for yt, yp in zip(y_true_all, y_pred_all)) / len(y_true_all)\n",
    "            accs.append(overall_accuracy)\n",
    "            runtimes.append((end_time - start_time) * 1000)  # ms\n",
    "            \n",
    "            # Calculate per-class accuracy for this run\n",
    "            class_accuracies = {}\n",
    "            for class_name in class_names:\n",
    "                class_true_indices = [i for i, yt in enumerate(y_true_all) if yt == class_name]\n",
    "                if class_true_indices:  # Only if class exists in test set\n",
    "                    class_correct = sum(1 for i in class_true_indices if y_pred_all[i] == class_name)\n",
    "                    class_accuracies[class_name] = class_correct / len(class_true_indices)\n",
    "                else:\n",
    "                    class_accuracies[class_name] = None  # No samples of this class\n",
    "            \n",
    "            class_accuracy_runs.append(class_accuracies)\n",
    "\n",
    "        # Average over num_runs runs\n",
    "        avg_acc = round(sum(accs) / num_runs, 4)\n",
    "        avg_rt = round(sum(runtimes) / num_runs, 3)\n",
    "        \n",
    "        # Average per-class accuracies over num_run runs\n",
    "        avg_class_accuracies = {}\n",
    "        for class_name in class_names:\n",
    "            class_accs = [run[class_name] for run in class_accuracy_runs if run[class_name] is not None]\n",
    "            if class_accs:\n",
    "                avg_class_accuracies[class_name] = round(sum(class_accs) / len(class_accs), 4)\n",
    "            else:\n",
    "                avg_class_accuracies[class_name] = None\n",
    "\n",
    "        # Build output row: feature columns + blank pads + avg metrics\n",
    "        row = list(combo) + [\"\"] * (total_features - len(combo)) + [avg_acc, avg_rt]\n",
    "        results.append(row)\n",
    "        \n",
    "        # Store per-class data for this feature combination\n",
    "        per_class_data.append({\n",
    "            'Feature_Combination': combo,\n",
    "            'Feature_Count': len(combo),\n",
    "            'Overall_Accuracy': avg_acc,\n",
    "            'Runtime_ms': avg_rt,\n",
    "            **{class_name: avg_class_accuracies[class_name] for class_name in class_names}\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úÖ Completed testing {len(results):,} feature combinations\")\n",
    "print(f\"Per-class data collected for {len(per_class_data)} feature combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2bfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Analysis Summary:\n",
      "   Model tested: AdaBoostClassifier\n",
      "   Feature combinations: 137\n",
      "   Best accuracy: 0.8611\n",
      "   Total runtime: 1083.9 seconds\n",
      "   Average runtime per combination: 7911.6 ms\n",
      "\n",
      "üíæ Results saved to: C:\\Misc\\ml_benchmark\\outputs\\results\\benchmark_adaboost_results_14_16.xlsx\n",
      "üíæ Per-class results saved to: C:\\Misc\\ml_benchmark\\outputs\\results\\benchmark_adaboost_per_class_14_16.xlsx\n",
      "\n",
      "Top 5 Feature Combinations:\n",
      "   0.8611 | 14 features | ['A', 'P', 'l', 'K', 'Ec', 'C', 'Ed', 'Ex', 'S', 'R', 'CO', 'SF1', 'SF3', 'SF4']\n",
      "   0.8545 | 14 features | ['A', 'L', 'l', 'K', 'Ec', 'C', 'Ed', 'Ex', 'S', 'R', 'CO', 'SF1', 'SF3', 'SF4']\n",
      "   0.8524 | 14 features | ['P', 'L', 'K', 'Ec', 'C', 'Ed', 'Ex', 'S', 'R', 'CO', 'SF1', 'SF2', 'SF3', 'SF4']\n",
      "   0.8524 | 14 features | ['A', 'P', 'L', 'K', 'C', 'Ed', 'Ex', 'S', 'R', 'CO', 'SF1', 'SF2', 'SF3', 'SF4']\n",
      "   0.8524 | 14 features | ['A', 'P', 'L', 'K', 'Ec', 'C', 'Ex', 'S', 'R', 'CO', 'SF1', 'SF2', 'SF3', 'SF4']\n",
      "\n",
      "Per-Class Accuracy Summary for AdaBoostClassifier:\n",
      "   0: 0.6718 (67.18%)\n",
      "   1: 0.2667 (26.67%)\n",
      "   2: 0.9006 (90.06%)\n",
      "   3: 0.9081 (90.81%)\n",
      "   4: 0.9203 (92.03%)\n",
      "   5: 0.9142 (91.42%)\n",
      "   6: 0.7853 (78.53%)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 5. ASSEMBLE AND STORE RESULTS IN TWO EXCEL FILES\n",
    "# -------------------------------------------\n",
    "\n",
    "# Create a results DataFrame with proper column structure.\n",
    "# Columns: Feature_1, Feature_2, ..., Feature_16, Accuracy, Runtime (ms)\n",
    "columns = [f\"Feature_{i+1}\" for i in range(total_features)] + [\"Accuracy\", \"Runtime (ms)\"]\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Sort by accuracy (descending) to see best performing combinations first\n",
    "results_df_sorted = results_df.sort_values(\"Accuracy\", ascending=False)\n",
    "\n",
    "\n",
    "# PREPARE AND SAVE FIRST EXCEL FILE: Accuracy and Runtime Results for each feature set\n",
    "# Generate the Excel output filename following the naming convention:\n",
    "# benchmark_{model_name}_results_{min_features}_{max_features}.xlsx\n",
    "\n",
    "model_name_clean = selected_model_name.lower().replace(\"classifier\", \"\")\n",
    "output_filename = f\"benchmark_{model_name_clean}_results_{min_features}_{total_features}.xlsx\"\n",
    "output_path = RESULTS_DIR / output_filename\n",
    "\n",
    "# Create and save the Excel file showing accuracy and runtime for each feature combination, for this model.\n",
    "results_df_sorted.to_excel(output_path, index=False)\n",
    "\n",
    "\n",
    "# PREAPARE AND SAVE SECOND EXCEL FILE: Per-Class Accuracy Summary for this model\n",
    "# Create per-class accuracy summary for this model (how well this model predicted the label's value)\n",
    "class_names = sorted(y.unique())\n",
    "\n",
    "# Calculate average per-class accuracy across all feature combinations\n",
    "model_per_class_summary = {'Model': selected_model_name}\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_accuracies = [data[class_name] for data in per_class_data if data[class_name] is not None]\n",
    "    if class_accuracies:\n",
    "        avg_class_acc = round(sum(class_accuracies) / len(class_accuracies), 4)\n",
    "        model_per_class_summary[f'Accuracy_{class_name}'] = avg_class_acc\n",
    "    else:\n",
    "        model_per_class_summary[f'Accuracy_{class_name}'] = None\n",
    "\n",
    "# Create per-class DataFrame (single row)\n",
    "per_class_df = pd.DataFrame([model_per_class_summary])\n",
    "\n",
    "# Generate per-class Excel filename\n",
    "per_class_filename = f\"benchmark_{model_name_clean}_per_class_{min_features}_{total_features}.xlsx\"\n",
    "per_class_path = RESULTS_DIR / per_class_filename\n",
    "\n",
    "# Create and then save the Excel file showing this model's accuracy in identifying each bean variety\n",
    "per_class_df.to_excel(per_class_path, index=False)\n",
    "\n",
    "# Display summary statistics\n",
    "best_accuracy = results_df_sorted['Accuracy'].iloc[0]\n",
    "total_runtime_sec = results_df['Runtime (ms)'].sum() / 1000\n",
    "avg_runtime_ms = results_df['Runtime (ms)'].mean()\n",
    "\n",
    "print(f\"\\nAnalysis Summary:\")\n",
    "print(f\"   Model tested: {selected_model_name}\")\n",
    "print(f\"   Feature combinations: {len(results):,}\")\n",
    "print(f\"   Best accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"   Total runtime: {total_runtime_sec:.1f} seconds\")\n",
    "print(f\"   Average runtime per combination: {avg_runtime_ms:.1f} ms\")\n",
    "print(f\"\\nResults saved to: {output_path}\")\n",
    "print(f\"Per-class results saved to: {per_class_path}\")\n",
    "\n",
    "# Display top 5 feature combinations\n",
    "print(f\"\\nTop 5 Feature Combinations:\")\n",
    "for i, row in results_df_sorted.head().iterrows():\n",
    "    features_used = [col for col in columns[:-2] if row[col] != \"\"]\n",
    "    feature_list = [row[col] for col in features_used]\n",
    "    print(f\"   {row['Accuracy']:.4f} | {len(feature_list)} features | {feature_list}\")\n",
    "\n",
    "# Display per-class accuracy summary\n",
    "print(f\"\\nPer-Class Accuracy Summary for {selected_model_name}:\")\n",
    "for class_name in class_names:\n",
    "    class_acc = model_per_class_summary[f'Accuracy_{class_name}']\n",
    "    if class_acc is not None:\n",
    "        print(f\"   {class_name}: {class_acc:.4f} ({class_acc*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"   {class_name}: No data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ae9fbc",
   "metadata": {},
   "source": [
    "Validation Methodology: Single Feature Set Analysis\n",
    "\n",
    "The validation section below employs a fundamentally different approach compared to the main benchmarking code. While the main analysis iterates through 137 different feature combinations (ranging from 14 to 16 features), in order to determine a more stable estimate of each model's runtime, the validation below focuses exclusively on the complete, original 16-feature dataset to provide a detailed accuracy baseline and confusion matrix analysis.\n",
    "\n",
    "Key Differences in Validation Approach:\n",
    "- **Single Feature Set**: Uses only the complete 16-feature dataset rather than testing multiple feature combinations\n",
    "- **Detailed Confusion Matrix**: Generates a comprehensive confusion matrix showing exactly which bean classes are confused with others\n",
    "- **Per-Class Accuracy Validation**: Calculates individual accuracy for each of the 7 bean varieties using the same cross-validation methodology as the main code\n",
    "- **Identical Cross-Validation**: Maintains the same 5-fold stratified cross-validation with random_state=42 for direct comparison with main results\n",
    "- **Comprehensive Reporting**: Creates detailed text reports and Excel files specifically for validation and model comparison purposes\n",
    "\n",
    "This validation approach serves to confirm the accuracy calculation methodology used in the main benchmark while providing deeper insights into model performance patterns that may be masked when averaging across 137 different feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d74506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION: Creating confusion matrix for GradientBoostingClassifier\n",
      "Using all 16 features on 13611 records\n",
      "This validates the per-class accuracy calculations\n",
      "\n",
      "Running cross-validation...\n",
      "\n",
      "VALIDATION SUMMARY:\n",
      "   Model: GradientBoostingClassifier\n",
      "   Overall Accuracy: 0.9269 (92.69%)\n",
      "\n",
      "Validation report saved to: C:\\Misc\\ml_benchmark\\outputs\\results\\benchmark_gradientboosting_validation_14_16.txt\n",
      "Report includes confusion matrix, per-class accuracy, and detailed statistics\n",
      "\n",
      "Validation complete! All results saved to text file.\n",
      "Added GradientBoostingClassifier sheet to existing Excel file: C:\\Misc\\ml_benchmark\\outputs\\results\\confusion_matrices_all_models_14_16.xlsx\n",
      "F1-scores Excel file created and saved: C:\\Misc\\ml_benchmark\\outputs\\results\\benchmark_gradientboosting_f1_scores_14_16.xlsx\n",
      "   F1-Macro: 0.9385, F1-Weighted: 0.9269\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 6. DETERMINE ACCURACY OF A SELECTED MODEL\n",
    "# -------------------------------------------\n",
    "\n",
    "# PURPOSE:\n",
    "# This code cell creates a confusion matrix and detailed accuracy report for the selected model to:\n",
    "#   1. Validate the per-class accuracy calculations from the main benchmark (prior Section 4)\n",
    "#   2. Provide detailed insight into which bean varieties the model predicts well/poorly\n",
    "#   3. Confirm the overall accuracy methodology by using only the complete, 16-feature, orginal dataset\n",
    "#   4. Generate a comprehensive text report saved alongside the Excel benchmark files\n",
    "#\n",
    "# APPROACH:\n",
    "# This code cell uses identical cross-validation methodology as the main benchmark, but focuses on:\n",
    "#   - Only the complete feature set (all 16 features) rather than iterating through other feature subsets\n",
    "#   - Detailed confusion matrix showing prediction patterns for each bean class\n",
    "#   - Per-class accuracy breakdown to identify model strengths/weaknesses\n",
    "#   - Comparison context for interpreting the averaged results in the Excel files\n",
    "#\n",
    "# OUTPUTS: \n",
    "#   1. Creates a detailed .txt validation report for the selected model in the same directory as Excel results\n",
    "#   2. Adds the selected model's confusion matrix to an existing Excel file containing all models' confusion matrices\n",
    "#   3. Creates an Excel file for the selected model with F1-scores and overall accuracy\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"\\nVALIDATION: Creating confusion matrix for {selected_model_name}\")\n",
    "print(f\"Using all {total_features} features on {len(X)} records\")\n",
    "print(f\"This validates the per-class accuracy calculations\")\n",
    "\n",
    "# Use the same cross-validation setup as the main analysis\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Collect all predictions using identical methodology\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "print(f\"\\nRunning cross-validation...\")\n",
    "\n",
    "# Manual cross-validation (identical to main code)\n",
    "for train_idx, test_idx in cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Use fresh model instance and train/predict (same as main code)\n",
    "    validation_model = model_dict[selected_model_name]\n",
    "    validation_model.fit(X_train, y_train)\n",
    "    y_pred = validation_model.predict(X_test)\n",
    "    \n",
    "    # Store all predictions\n",
    "    y_true_all.extend(y_test.tolist())\n",
    "    y_pred_all.extend(y_pred.tolist())\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "\n",
    "# Create readable confusion matrix with class labels\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                     index=[f\"True_{cls}\" for cls in sorted(y.unique())], \n",
    "                     columns=[f\"Pred_{cls}\" for cls in sorted(y.unique())])\n",
    "\n",
    "# Calculate per-class accuracy from confusion matrix\n",
    "validation_per_class = {}\n",
    "for i, class_name in enumerate(sorted(y.unique())):\n",
    "    class_total = cm[i, :].sum()  # Total actual samples of this class\n",
    "    class_correct = cm[i, i]      # Correctly predicted samples\n",
    "    class_accuracy = class_correct / class_total if class_total > 0 else 0\n",
    "    validation_per_class[class_name] = class_accuracy\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = sum(y_true_all[i] == y_pred_all[i] for i in range(len(y_true_all))) / len(y_true_all)\n",
    "\n",
    "# Identify perfect and poor performing classes\n",
    "perfect_classes = [cls for cls, acc in validation_per_class.items() if acc == 1.0]\n",
    "poor_classes = [cls for cls, acc in validation_per_class.items() if acc < 0.8]\n",
    "\n",
    "# Generate validation report filename\n",
    "model_name_clean = selected_model_name.lower().replace(\"classifier\", \"\")\n",
    "validation_filename = f\"benchmark_{model_name_clean}_validation_{min_features}_{total_features}.txt\"\n",
    "validation_path = RESULTS_DIR / validation_filename\n",
    "\n",
    "# Create comprehensive text report\n",
    "report_content = []\n",
    "report_content.append(\"=\" * 80)\n",
    "report_content.append(f\"VALIDATION REPORT: {selected_model_name}\")\n",
    "report_content.append(\"=\" * 80)\n",
    "report_content.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "report_content.append(f\"Dataset: {len(X):,} records, {total_features} features\")\n",
    "report_content.append(f\"Validation uses all {total_features} features (complete feature set)\")\n",
    "report_content.append(\"\")\n",
    "\n",
    "# Confusion Matrix Section\n",
    "report_content.append(\"CONFUSION MATRIX\")\n",
    "report_content.append(\"-\" * 40)\n",
    "report_content.append(\"Rows = True Bean Classes, Columns = Predicted Bean Classes\")\n",
    "report_content.append(f\"Bean Classes: {sorted(y.unique())}\")\n",
    "report_content.append(\"\")\n",
    "report_content.append(cm_df.to_string())\n",
    "report_content.append(\"\")\n",
    "\n",
    "# Per-Class Accuracy Section\n",
    "report_content.append(\"PER-CLASS ACCURACY (from confusion matrix)\")\n",
    "report_content.append(\"-\" * 50)\n",
    "for i, class_name in enumerate(sorted(y.unique())):\n",
    "    class_total = cm[i, :].sum()\n",
    "    class_correct = cm[i, i]\n",
    "    class_accuracy = validation_per_class[class_name]\n",
    "    report_content.append(f\"   Bean Class {class_name}: {class_accuracy:.4f} ({class_accuracy*100:.2f}%) - {class_correct}/{class_total} correct\")\n",
    "\n",
    "report_content.append(\"\")\n",
    "\n",
    "# Overall Accuracy Section\n",
    "report_content.append(\"OVERALL PERFORMANCE\")\n",
    "report_content.append(\"-\" * 30)\n",
    "report_content.append(f\"Overall Accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
    "report_content.append(\"\")\n",
    "\n",
    "# Performance Classification\n",
    "if perfect_classes:\n",
    "    report_content.append(f\"Perfect Classification (100%): Bean classes {perfect_classes}\")\n",
    "if poor_classes:\n",
    "    report_content.append(f\"Challenging Classification (<80%): Bean classes {poor_classes}\")\n",
    "if not perfect_classes and not poor_classes:\n",
    "    report_content.append(\"All classes achieved 80%+ accuracy, none achieved 100%\")\n",
    "\n",
    "report_content.append(\"\")\n",
    "\n",
    "# Comparison to Main Results\n",
    "report_content.append(\"COMPARISON TO MAIN BENCHMARK RESULTS\")\n",
    "report_content.append(\"-\" * 45)\n",
    "report_content.append(\"   ‚Ä¢ These per-class accuracies are for the complete 16-feature set only\")\n",
    "report_content.append(\"   ‚Ä¢ Your main benchmark Excel file shows averages across all 137 feature combinations\")\n",
    "report_content.append(\"   ‚Ä¢ Expected: These validation results may differ slightly from main benchmark averages\")\n",
    "report_content.append(\"   ‚Ä¢ This validation confirms the accuracy calculation methodology\")\n",
    "report_content.append(\"\")\n",
    "\n",
    "# Additional Statistics\n",
    "report_content.append(\"DETAILED STATISTICS\")\n",
    "report_content.append(\"-\" * 25)\n",
    "report_content.append(f\"Cross-validation folds: 5\")\n",
    "report_content.append(f\"Random state: 42 (reproducible results)\")\n",
    "report_content.append(f\"Total predictions made: {len(y_true_all):,}\")\n",
    "report_content.append(f\"Correct predictions: {sum(y_true_all[i] == y_pred_all[i] for i in range(len(y_true_all))):,}\")\n",
    "report_content.append(f\"Incorrect predictions: {len(y_true_all) - sum(y_true_all[i] == y_pred_all[i] for i in range(len(y_true_all))):,}\")\n",
    "\n",
    "# Class distribution\n",
    "report_content.append(\"\")\n",
    "report_content.append(\"CLASS DISTRIBUTION IN VALIDATION\")\n",
    "report_content.append(\"-\" * 35)\n",
    "for class_name in sorted(y.unique()):\n",
    "    class_count = y_true_all.count(class_name)\n",
    "    class_percentage = (class_count / len(y_true_all)) * 100\n",
    "    report_content.append(f\"   Bean Class {class_name}: {class_count:,} samples ({class_percentage:.1f}%)\")\n",
    "\n",
    "report_content.append(\"\")\n",
    "report_content.append(\"=\" * 80)\n",
    "report_content.append(\"END OF VALIDATION REPORT\")\n",
    "report_content.append(\"=\" * 80)\n",
    "\n",
    "# Write to file\n",
    "with open(validation_path, 'w') as f:\n",
    "    f.write('\\n'.join(report_content))\n",
    "\n",
    "# Display summary to console\n",
    "print(f\"\\nVALIDATION SUMMARY:\")\n",
    "print(f\"   Model: {selected_model_name}\")\n",
    "print(f\"   Overall Accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
    "if perfect_classes:\n",
    "    print(f\"   Perfect Classes: {perfect_classes}\")\n",
    "if poor_classes:\n",
    "    print(f\"   ‚ö†Ô∏è  Challenging Classes: {poor_classes}\")\n",
    "\n",
    "print(f\"\\nValidation report saved to: {validation_path}\")\n",
    "print(f\"Report includes confusion matrix, per-class accuracy, and detailed statistics\")\n",
    "print(f\"\\nValidation complete! All results saved to text file.\")\n",
    "\n",
    "\n",
    "# Save confusion matrix to Excel format for consistency analysis\n",
    "cm_excel_filename = f\"confusion_matrices_all_models_{min_features}_{total_features}.xlsx\"\n",
    "cm_excel_path = RESULTS_DIR / cm_excel_filename\n",
    "\n",
    "# Add model name column to confusion matrix\n",
    "cm_df_excel = cm_df.copy()\n",
    "cm_df_excel.insert(0, 'Model', selected_model_name)\n",
    "\n",
    "# Check if Excel file already exists\n",
    "if cm_excel_path.exists():\n",
    "    # File exists - add new sheet to existing workbook\n",
    "    with pd.ExcelWriter(cm_excel_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "        cm_df_excel.to_excel(writer, sheet_name=selected_model_name, index=True)\n",
    "    print(f\"Added {selected_model_name} sheet to existing Excel file: {cm_excel_path}\")\n",
    "else:\n",
    "    # File doesn't exist - create new workbook with first sheet\n",
    "    cm_df_excel.to_excel(cm_excel_path, sheet_name=selected_model_name, index=True)\n",
    "    print(f\"Created new Excel file with {selected_model_name} sheet: {cm_excel_path}\")\n",
    "\n",
    "\n",
    "# Calculate and save F1-scores for this model\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1-scores using existing predictions from validation\n",
    "f1_macro = f1_score(y_true_all, y_pred_all, average='macro')\n",
    "f1_weighted = f1_score(y_true_all, y_pred_all, average='weighted')\n",
    "\n",
    "# Create F1-scores summary (single row)\n",
    "f1_summary = {\n",
    "    'Model': selected_model_name,\n",
    "    'F1_Macro': round(f1_macro, 4),\n",
    "    'F1_Weighted': round(f1_weighted, 4),\n",
    "    'Overall_Accuracy': round(overall_accuracy, 4)\n",
    "}\n",
    "\n",
    "# Create F1-scores DataFrame and save to Excel\n",
    "f1_df = pd.DataFrame([f1_summary])\n",
    "f1_filename = f\"benchmark_{model_name_clean}_f1_scores_{min_features}_{total_features}.xlsx\"\n",
    "f1_path = RESULTS_DIR / f1_filename\n",
    "f1_df.to_excel(f1_path, index=False)\n",
    "\n",
    "# Confirmation print with full path\n",
    "print(f\"F1-scores Excel file created and saved: {f1_path}\")\n",
    "print(f\"   F1-Macro: {f1_macro:.4f}, F1-Weighted: {f1_weighted:.4f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e2a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consolidating F1-scores from 16 models...\n",
      "   Added adaboost F1-scores to consolidated file\n",
      "   Added decisiontree F1-scores to consolidated file\n",
      "   Added extratrees F1-scores to consolidated file\n",
      "   Added gaussiannb F1-scores to consolidated file\n",
      "   Added gradientboosting F1-scores to consolidated file\n",
      "   Added kneighbors F1-scores to consolidated file\n",
      "   Added lineardiscriminantanalysis F1-scores to consolidated file\n",
      "   Added logisticregression F1-scores to consolidated file\n",
      "   Added mlp F1-scores to consolidated file\n",
      "   Added perceptron F1-scores to consolidated file\n",
      "   Added quadraticdiscriminantanalysis F1-scores to consolidated file\n",
      "   Added randomforest F1-scores to consolidated file\n",
      "   Added ridge F1-scores to consolidated file\n",
      "   Added sgd F1-scores to consolidated file\n",
      "   Added svc F1-scores to consolidated file\n",
      "   Added xgb F1-scores to consolidated file\n",
      "\n",
      "‚úÖ F1-scores consolidated successfully: C:\\Misc\\ml_benchmark\\outputs\\results\\f1_scores_all_models_14_16.xlsx\n",
      "   Total sheets: 16\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 7. CONSOLIDATE F1-SCORES FROM ALL MODELS INTO SINGLE EXCEL FILE\n",
    "# -------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define search pattern for F1-score files\n",
    "f1_pattern = f\"benchmark_*_f1_scores_{min_features}_{total_features}.xlsx\"\n",
    "f1_files = list(RESULTS_DIR.glob(f1_pattern))    # Uses the .glob() method of the pathlib.Path object, so glob import not needed\n",
    "\n",
    "# Create consolidated F1-scores Excel file\n",
    "consolidated_f1_filename = f\"f1_scores_all_models_{min_features}_{total_features}.xlsx\"\n",
    "consolidated_f1_path = RESULTS_DIR / consolidated_f1_filename\n",
    "\n",
    "if f1_files:\n",
    "    print(f\"\\nConsolidating F1-scores from {len(f1_files)} models...\")\n",
    "    \n",
    "    with pd.ExcelWriter(consolidated_f1_path, engine='openpyxl') as writer:\n",
    "        for f1_file in f1_files:\n",
    "            # Extract model name from filename\n",
    "            model_name = f1_file.stem.replace(f\"benchmark_\", \"\").replace(f\"_f1_scores_{min_features}_{total_features}\", \"\")\n",
    "            \n",
    "            # Read the F1-score data\n",
    "            f1_data = pd.read_excel(f1_file)\n",
    "            \n",
    "            # Write to sheet named after model\n",
    "            f1_data.to_excel(writer, sheet_name=model_name, index=False)\n",
    "            \n",
    "            print(f\"   Added {model_name} F1-scores to consolidated file\")\n",
    "    \n",
    "    print(f\"\\nF1-scores consolidated successfully: {consolidated_f1_path}\")\n",
    "    print(f\"   Total sheets: {len(f1_files)}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No F1-score files found matching pattern: {f1_pattern}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
