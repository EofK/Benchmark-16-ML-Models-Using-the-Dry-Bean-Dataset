{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b0bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 16 baseline models with default parameters\n",
      "Data source: C:\\Misc\\ml_benchmark\\outputs\\curated_data\\DryBean_curated.parquet\n",
      "Results will be saved to: C:\\Misc\\ml_benchmark\\outputs\\baseline_results\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 1. IMPORT, DEFINE PATHS, AND CREATE DEFAULT MODEL DICTIONARY\n",
    "# -------------------------------------------\n",
    "\n",
    "# Stage 04b: Baseline Model Evaluation (Before Hyperparameter Tuning)\n",
    "\n",
    "# The objective of this module 04b is to measure the accuracy and runtime for each model\n",
    "# using only default parameters (i.e., no hyperparameter tuning). This provides the BEFORE\n",
    "# measures of each model's accuracy and runtime.\n",
    "\n",
    "# This stage creates default model instances with no hyperparameter tuning and performs exhaustive\n",
    "# feature subset analysis on a selected model. It generates all possible feature\n",
    "# combinations from min_features (e.g., 14) to total_features (16) and evaluates baseline model performance\n",
    "# on each combination. This stage creates and saves results to six different output files:\n",
    "#\n",
    "# PER-MODEL FILES (created for each individual model tested):\n",
    "#    1. Main results file: baseline_{model}_results_{min}_{max}.xlsx \n",
    "#       - Shows accuracy and runtime for each feature combination (137 rows)\n",
    "#    2. Per-class accuracy: baseline_{model}_per_class_{min}_{max}.xlsx\n",
    "#       - Shows how well the model predicted each bean variety (1 row)\n",
    "#    3. Validation report: baseline_{model}_validation_{min}_{max}.txt\n",
    "#       - Detailed confusion matrix and accuracy analysis for complete feature set\n",
    "#    4. F1-scores: baseline_{model}_f1_scores_{min}_{max}.xlsx\n",
    "#       - F1-macro, F1-weighted, and overall accuracy metrics (1 row)\n",
    "#\n",
    "# CONSOLIDATED FILES (created/updated after each model run):\n",
    "#    5. Confusion matrices: baseline_confusion_matrices_all_models_{min}_{max}.xlsx\n",
    "#       - Each model's confusion matrix as a separate sheet\n",
    "#    6. All F1-scores: baseline_f1_scores_all_models_{min}_{max}.xlsx\n",
    "#       - F1-scores from all baseline models consolidated into one file\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import all model classes for default_model_dict\n",
    "# Individual model imports are used only in this 04b module. Dynamic imports are used\n",
    "# in downstream modules. (This module could be changed in a future version to dynamically import.)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define this project's file locations.\n",
    "# This notebook uses a centralized config.py file for all path management.\n",
    "\n",
    "# Import config paths\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import TUNED_MODELS_DIR, CURATED_DATA_DIR, RESULTS_DIR, BASELINE_RESULTS_DIR\n",
    "\n",
    "curated_data_dir = CURATED_DATA_DIR\n",
    "results_dir = BASELINE_RESULTS_DIR   # Use dedicated baseline results directory\n",
    "tuned_models_dir = TUNED_MODELS_DIR  # NOTE: This is not used in this module, but kept for consistency\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define file paths\n",
    "data_path = CURATED_DATA_DIR / \"DryBean_curated.parquet\"    # From Stage 04\n",
    "\n",
    "# Create default model dictionary with baseline parameters (no hyperparameter tuning)\n",
    "# Using the same 16-model order established in Stage 05\n",
    "default_model_dict = {\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(),\n",
    "    'XGBClassifier': XGBClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'RidgeClassifier': RidgeClassifier(),\n",
    "    'SGDClassifier': SGDClassifier(),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'SVC': SVC(),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'LinearDiscriminantAnalysis': LinearDiscriminantAnalysis(),\n",
    "    'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n",
    "    'MLPClassifier': MLPClassifier()\n",
    "}\n",
    "\n",
    "print(f\"Created {len(default_model_dict)} baseline models with default parameters\")\n",
    "print(f\"Data source: {data_path}\")\n",
    "print(f\"Results will be saved to: {results_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d245ba6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Available baseline models (with default parameters):\n",
      "    1. DecisionTreeClassifier\n",
      "    2. RandomForestClassifier\n",
      "    3. ExtraTreesClassifier\n",
      "    4. GradientBoostingClassifier\n",
      "    5. AdaBoostClassifier\n",
      "    6. XGBClassifier\n",
      "    7. LogisticRegression\n",
      "    8. RidgeClassifier\n",
      "    9. SGDClassifier\n",
      "   10. Perceptron\n",
      "   11. SVC\n",
      "   12. KNeighborsClassifier\n",
      "   13. GaussianNB\n",
      "   14. LinearDiscriminantAnalysis\n",
      "   15. QuadraticDiscriminantAnalysis\n",
      "   16. MLPClassifier\n",
      "\n",
      "‚úÖ Selected baseline model: GradientBoostingClassifier\n",
      "üîç Testing feature set combinations from 14 to all features\n",
      "Model parameters:\n",
      "   ccp_alpha=0.0, criterion=friedman_mse, init=None, learning_rate=0.1, loss=log_loss\n",
      "   max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=1\n",
      "   min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=None\n",
      "   subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 2. SELECT MODEL AND FEATURE SETS\n",
    "# -------------------------------------------\n",
    "\n",
    "# Display available baseline models in the order defined in default_model_dict\n",
    "print(\"\\n Available baseline models (with default parameters):\")\n",
    "for i, model_name in enumerate(default_model_dict.keys(), 1):\n",
    "    print(f\"   {i:2d}. {model_name}\")\n",
    "\n",
    "\n",
    "# ------------- SELECT MODEL TO TEST HERE -------------\n",
    "# Change this variable to select the model to test\n",
    "selected_model_name = \"GradientBoostingClassifier\"  # Change this to test different models\n",
    "# -----------------------------------------------------\n",
    "\n",
    "if selected_model_name not in default_model_dict:\n",
    "    print(f\"‚ùå Error: {selected_model_name} not found in default_model_dict\")\n",
    "    print(f\"Available models: {list(default_model_dict.keys())}\")\n",
    "else:\n",
    "    model = default_model_dict[selected_model_name]\n",
    "    print(f\"\\n‚úÖ Selected baseline model: {selected_model_name}\")\n",
    "    \n",
    "    # Feature Selection Parameters\n",
    "    # ------ ENTER THE MINIMUM FEATURE COUNT HERE -------\n",
    "    min_features = 14 \n",
    "    print(f\"üîç Testing feature set combinations from {min_features} to all features\")\n",
    "    # ---------------------------------------------------\n",
    "\n",
    "    # Display the model's parameters\n",
    "    params = model.get_params()\n",
    "    param_items = list(params.items())\n",
    "    print(f\"Model parameters:\")\n",
    "    for i in range(0, len(param_items), 5):\n",
    "        chunk = param_items[i:i+5]\n",
    "        param_str = \", \".join([f\"{k}={v}\" for k, v in chunk])\n",
    "        print(f\"   {param_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bbade755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset Information:\n",
      "   Shape: (13611, 16)\n",
      "   Total features: 16\n",
      "   Feature names: ['A', 'P', 'L', 'l', 'K', 'Ec', 'C', 'Ed', 'Ex', 'S', 'R', 'CO', 'SF1', 'SF2', 'SF3', 'SF4']\n",
      "   Target classes: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "   Data types: Features=float64, Target=int64\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 3. LOAD PREPROCESSED DATA\n",
    "# -------------------------------------------\n",
    "\n",
    "# Load processed dataset from Stage 04 (scaled and encoded)\n",
    "df = pd.read_parquet(data_path)\n",
    "X = df.drop(columns=[\"label\"])  # Features (already scaled/encoded from Stage 04)\n",
    "y = df[\"label\"]                 # Target (already label-encoded from Stage 04)\n",
    "\n",
    "# The data was preprocessed in Stage 04, so can be used here\n",
    "# without additional scaling or encoding.\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "total_features = len(feature_names)\n",
    "unique_classes = y.unique()\n",
    "\n",
    "# Confirm key dataset information. The last print line confirmas that: \n",
    "#   1. Target classes are numeric (some models require numeric labels), and\n",
    "#   2. No classes are missing.\n",
    "print(f\"\\n Dataset Information:\")\n",
    "print(f\"   Shape: {X.shape}\")\n",
    "print(f\"   Total features: {total_features}\")\n",
    "print(f\"   Feature names: {feature_names}\")\n",
    "print(f\"   Target classes: {sorted(unique_classes)}\")\n",
    "print(f\"   Data types: Features={X.dtypes.iloc[0]}, Target={y.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90602647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bean varieties to track: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "\n",
      "üîÑ Starting exhaustive feature subset analysis...\n",
      "   Total combinations to test: 137\n",
      "   Feature range: 14 to 16 features\n",
      "\n",
      "üß™ Testing 120 combinations of 14 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature combinations (14 features): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120/120 [4:40:45<00:00, 140.38s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing 16 combinations of 15 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature combinations (15 features): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [40:06<00:00, 150.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing 1 combinations of 16 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature combinations (16 features): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [02:39<00:00, 159.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Completed testing 137 feature combinations\n",
      "Per-class data collected for 137 feature combinations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 4. MEASURE AND RECORD A MODEL'S RUNTIMES (3-LOOP AVERAGE). LATEST VERSION!\n",
    "# -------------------------------------------\n",
    "\n",
    "# This code block creates all possible features sets, then loops through tests of the selected model \n",
    "# on every feature set created.  Each feature set is examined the number of times \n",
    "# set by the user in the variable num_runs. Meaning, each feature set can be examined 1, 2, 3,... times.\n",
    "# The purpose of examining each feature set more than once is to use the average runtime of all loops,\n",
    "# possibly offseting runtime outliers caused by Windows background processes and RAM management\n",
    "# interfering with the model's runtime.\n",
    "#  \n",
    "# The average accuracy and runtime across all three loops of the same feature set are recorded.\n",
    "# Additionally, we measure how well the model predicts each of the 7 bean varieties.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "per_class_data = []  # Each model's accuracy in predicing each bean variety (aka, class) is stored here.\n",
    "\n",
    "# Loops taken on each dataset record. Used to possibly offset runtime outliers caused\n",
    "# by Windows background processes and RAM management interfering with the model's runtime.\n",
    "num_runs = 1\n",
    "\n",
    "# Get unique class names for consistent reporting\n",
    "class_names = sorted(y.unique())\n",
    "print(f\"Bean varieties to track: {class_names}\")\n",
    "\n",
    "# Calculate expected combinations for progress tracking.\n",
    "total_combinations = sum(len(list(itertools.combinations(feature_names, r))) \n",
    "                         for r in range(min_features, total_features + 1))\n",
    "print(f\"\\nStarting exhaustive feature subset analysis...\")\n",
    "print(f\"   Total combinations to test: {total_combinations:,}\")\n",
    "print(f\"   Feature range: {min_features} to {total_features} features\")\n",
    "\n",
    "# Conduct benchmark: repeat each feature set 3x, then determine average accuracy and runtime\n",
    "for r in range(min_features, len(feature_names) + 1):\n",
    "    combos = list(itertools.combinations(feature_names, r))\n",
    "    print(f\"\\nTesting {len(combos):,} combinations of {r} features...\")\n",
    "\n",
    "    for combo in tqdm(combos, desc=f\"Feature combinations ({r} features)\"):\n",
    "        accs = []\n",
    "        runtimes = []\n",
    "        \n",
    "        # Track per-class accuracy across all runs\n",
    "        class_accuracy_runs = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            model = default_model_dict[selected_model_name]  # fresh model each run\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            # Collect predictions for per-class analysis\n",
    "            y_true_all = []\n",
    "            y_pred_all = []\n",
    "            \n",
    "            # Manual cross-validation to capture individual predictions - generally same runtime for all models\n",
    "            for train_idx, test_idx in cv.split(X[list(combo)], y):\n",
    "                X_train, X_test = X[list(combo)].iloc[train_idx], X[list(combo)].iloc[test_idx]\n",
    "                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "                \n",
    "                \n",
    "                # Train model on 80% of dataset - COMPLETELY DIFFERENT for each model, so different runtimes\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Generate predictions on 20% of dataset - COMPLETELY DIFFERENT for each model, so different\n",
    "                # runtimes and accuracies\n",
    "                y_pred = model.predict(X_test)   \n",
    "                \n",
    "                # Store for per-class analysis - generally same runtime for all models\n",
    "                y_true_all.extend(y_test.tolist())\n",
    "                y_pred_all.extend(y_pred.tolist())\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            # Calculate overall accuracy\n",
    "            overall_accuracy = sum(yt == yp for yt, yp in zip(y_true_all, y_pred_all)) / len(y_true_all)\n",
    "            accs.append(overall_accuracy)\n",
    "            runtimes.append((end_time - start_time) * 1000)  # ms\n",
    "            \n",
    "            # Calculate per-class accuracy for this run\n",
    "            class_accuracies = {}\n",
    "            for class_name in class_names:\n",
    "                class_true_indices = [i for i, yt in enumerate(y_true_all) if yt == class_name]\n",
    "                if class_true_indices:  # Only if class exists in test set\n",
    "                    class_correct = sum(1 for i in class_true_indices if y_pred_all[i] == class_name)\n",
    "                    class_accuracies[class_name] = class_correct / len(class_true_indices)\n",
    "                else:\n",
    "                    class_accuracies[class_name] = None  # No samples of this class\n",
    "            \n",
    "            class_accuracy_runs.append(class_accuracies)\n",
    "\n",
    "        # Average over num_runs runs\n",
    "        avg_acc = round(sum(accs) / num_runs, 4)\n",
    "        avg_rt = round(sum(runtimes) / num_runs, 3)\n",
    "        \n",
    "        # Average per-class accuracies over num_run runs\n",
    "        avg_class_accuracies = {}\n",
    "        for class_name in class_names:\n",
    "            class_accs = [run[class_name] for run in class_accuracy_runs if run[class_name] is not None]\n",
    "            if class_accs:\n",
    "                avg_class_accuracies[class_name] = round(sum(class_accs) / len(class_accs), 4)\n",
    "            else:\n",
    "                avg_class_accuracies[class_name] = None\n",
    "\n",
    "        # Build output row: feature columns + blank pads + avg metrics\n",
    "        row = list(combo) + [\"\"] * (total_features - len(combo)) + [avg_acc, avg_rt]\n",
    "        results.append(row)\n",
    "        \n",
    "        # Store per-class data for this feature combination\n",
    "        per_class_data.append({\n",
    "            'Feature_Combination': combo,\n",
    "            'Feature_Count': len(combo),\n",
    "            'Overall_Accuracy': avg_acc,\n",
    "            'Runtime_ms': avg_rt,\n",
    "            **{class_name: avg_class_accuracies[class_name] for class_name in class_names}\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úÖ Completed testing {len(results):,} feature combinations\")\n",
    "print(f\"Per-class data collected for {len(per_class_data)} feature combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2bfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Analysis Summary:\n",
      "   Model tested: GradientBoostingClassifier\n",
      "   Feature combinations: 137\n",
      "   Best accuracy: 0.9268\n",
      "   Total runtime: 19411.2 seconds\n",
      "   Average runtime per combination: 141687.6 ms\n",
      "\n",
      "üíæ Results saved to: C:\\Misc\\ml_benchmark\\outputs\\baseline_results\\baseline_gradientboosting_results_14_16.xlsx\n",
      "üíæ Per-class results saved to: C:\\Misc\\ml_benchmark\\outputs\\baseline_results\\baseline_gradientboosting_per_class_14_16.xlsx\n",
      "\n",
      "Top 5 Feature Combinations:\n",
      "   0.9268 | 14 features | ['A', 'P', 'L', 'l', 'Ec', 'C', 'Ex', 'S', 'R', 'CO', 'SF1', 'SF2', 'SF3', 'SF4']\n",
      "   0.9265 | 14 features | ['A', 'L', 'l', 'K', 'Ec', 'C', 'Ed', 'Ex', 'S', 'R', 'CO', 'SF2', 'SF3', 'SF4']\n",
      "   0.9265 | 15 features | ['P', 'L', 'l', 'K', 'Ec', 'C', 'Ed', 'Ex', 'S', 'R', 'CO', 'SF1', 'SF2', 'SF3', 'SF4']\n",
      "   0.9265 | 16 features | ['A', 'P', 'L', 'l', 'K', 'Ec', 'C', 'Ed', 'Ex', 'S', 'R', 'CO', 'SF1', 'SF2', 'SF3', 'SF4']\n",
      "   0.9265 | 14 features | ['P', 'L', 'l', 'K', 'Ec', 'C', 'Ed', 'Ex', 'S', 'R', 'SF1', 'SF2', 'SF3', 'SF4']\n",
      "\n",
      "Per-Class Accuracy Summary for GradientBoostingClassifier:\n",
      "   0: 0.9081 (90.81%)\n",
      "   1: 0.9964 (99.64%)\n",
      "   2: 0.9399 (93.99%)\n",
      "   3: 0.9304 (93.04%)\n",
      "   4: 0.9508 (95.08%)\n",
      "   5: 0.9424 (94.24%)\n",
      "   6: 0.8636 (86.36%)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 5. ASSEMBLE AND STORE RESULTS IN TWO EXCEL FILES\n",
    "# -------------------------------------------\n",
    "\n",
    "# Create a results DataFrame with proper column structure.\n",
    "# Columns: Feature_1, Feature_2, ..., Feature_16, Accuracy, Runtime (ms)\n",
    "columns = [f\"Feature_{i+1}\" for i in range(total_features)] + [\"Accuracy\", \"Runtime (ms)\"]\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Sort by accuracy (descending) to see best performing combinations first\n",
    "results_df_sorted = results_df.sort_values(\"Accuracy\", ascending=False)\n",
    "\n",
    "\n",
    "# PREPARE AND SAVE FIRST EXCEL FILE: Accuracy and Runtime Results for each feature set\n",
    "# Generate the Excel output filename following the naming convention:\n",
    "# baseline_{model_name}_results_{min_features}_{max_features}.xlsx\n",
    "model_name_clean = selected_model_name.lower().replace(\"classifier\", \"\")\n",
    "output_filename = f\"baseline_{model_name_clean}_results_{min_features}_{total_features}.xlsx\"\n",
    "output_path = results_dir / output_filename\n",
    "\n",
    "# Create and save the Excel file showing accuracy and runtime for each feature combination, for this model.\n",
    "results_df_sorted.to_excel(output_path, index=False)\n",
    "\n",
    "\n",
    "# PREAPARE AND SAVE SECOND EXCEL FILE: Per-Class Accuracy Summary for this model\n",
    "# Create per-class accuracy summary for this model (how well this model predicted the label's value)\n",
    "class_names = sorted(y.unique())\n",
    "\n",
    "# Calculate average per-class accuracy across all feature combinations\n",
    "model_per_class_summary = {'Model': selected_model_name}\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_accuracies = [data[class_name] for data in per_class_data if data[class_name] is not None]\n",
    "    if class_accuracies:\n",
    "        avg_class_acc = round(sum(class_accuracies) / len(class_accuracies), 4)\n",
    "        model_per_class_summary[f'Accuracy_{class_name}'] = avg_class_acc\n",
    "    else:\n",
    "        model_per_class_summary[f'Accuracy_{class_name}'] = None\n",
    "\n",
    "# Create per-class DataFrame (single row)\n",
    "per_class_df = pd.DataFrame([model_per_class_summary])\n",
    "\n",
    "# Generate per-class Excel filename\n",
    "per_class_filename = f\"baseline_{model_name_clean}_per_class_{min_features}_{total_features}.xlsx\"\n",
    "per_class_path = results_dir / per_class_filename\n",
    "\n",
    "# Create and then save the Excel file showing this model's accuracy in identifying each bean variety\n",
    "per_class_df.to_excel(per_class_path, index=False)\n",
    "\n",
    "# Display summary statistics\n",
    "best_accuracy = results_df_sorted['Accuracy'].iloc[0]\n",
    "total_runtime_sec = results_df['Runtime (ms)'].sum() / 1000\n",
    "avg_runtime_ms = results_df['Runtime (ms)'].mean()\n",
    "\n",
    "print(f\"\\nAnalysis Summary:\")\n",
    "print(f\"   Model tested: {selected_model_name}\")\n",
    "print(f\"   Feature combinations: {len(results):,}\")\n",
    "print(f\"   Best accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"   Total runtime: {total_runtime_sec:.1f} seconds\")\n",
    "print(f\"   Average runtime per combination: {avg_runtime_ms:.1f} ms\")\n",
    "print(f\"\\nResults saved to: {output_path}\")\n",
    "print(f\"Per-class results saved to: {per_class_path}\")\n",
    "\n",
    "# Display top 5 feature combinations\n",
    "print(f\"\\nTop 5 Feature Combinations:\")\n",
    "for i, row in results_df_sorted.head().iterrows():\n",
    "    features_used = [col for col in columns[:-2] if row[col] != \"\"]\n",
    "    feature_list = [row[col] for col in features_used]\n",
    "    print(f\"   {row['Accuracy']:.4f} | {len(feature_list)} features | {feature_list}\")\n",
    "\n",
    "# Display per-class accuracy summary\n",
    "print(f\"\\nPer-Class (bean type) Accuracy Summary for {selected_model_name}:\")\n",
    "for class_name in class_names:\n",
    "    class_acc = model_per_class_summary[f'Accuracy_{class_name}']\n",
    "    if class_acc is not None:\n",
    "        print(f\"   {class_name}: {class_acc:.4f} ({class_acc*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"   {class_name}: No data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "78d74506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION: Creating confusion matrix for GradientBoostingClassifier\n",
      "Using all 16 features on 13611 records\n",
      "This validates the per-class accuracy calculations\n",
      "\n",
      "Running cross-validation...\n",
      "\n",
      "VALIDATION SUMMARY:\n",
      "   Model: GradientBoostingClassifier\n",
      "   Overall Accuracy: 0.9259 (92.59%)\n",
      "\n",
      "Validation report saved to: C:\\Misc\\ml_benchmark\\outputs\\baseline_results\\baseline_gradientboosting_validation_14_16.txt\n",
      "Report includes confusion matrix, per-class accuracy, and detailed statistics\n",
      "\n",
      "Validation complete! All results saved to text file.\n",
      "Added GradientBoostingClassifier sheet to existing Excel file: C:\\Misc\\ml_benchmark\\outputs\\baseline_results\\baseline_confusion_matrices_all_models_14_16.xlsx\n",
      "F1-scores Excel file created and saved: C:\\Misc\\ml_benchmark\\outputs\\baseline_results\\baseline_gradientboosting_f1_scores_14_16.xlsx\n",
      "   F1-Macro: 0.9375, F1-Weighted: 0.9259\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 6. VALIDATE RESULTS WITH A CM FOR SELECTED MODEL   ************* LATEST VERSION *****************\n",
    "# -------------------------------------------\n",
    "\n",
    "# VALIDATION PURPOSE:\n",
    "# This validation section creates a confusion matrix and detailed accuracy report to:\n",
    "#   1. Validate the per-class accuracy calculations from the main benchmark (prior Section 4)\n",
    "#   2. Provide detailed insight into which bean varieties the model predicts well/poorly\n",
    "#   3. Confirm the overall accuracy methodology by using only the complete, 16-feature, orginal dataset\n",
    "#   4. Generate a comprehensive text report saved alongside the Excel benchmark files\n",
    "#\n",
    "# VALIDATION APPROACH:\n",
    "# Uses identical cross-validation methodology as the main benchmark, but focuses on:\n",
    "#   - The complete feature set (all 16 features) rather than iterating through other feature subsets\n",
    "#   - Detailed confusion matrix showing prediction patterns for each bean class\n",
    "#   - Per-class accuracy breakdown to identify model strengths/weaknesses\n",
    "#   - Comparison context for interpreting the averaged results in the Excel files\n",
    "#\n",
    "# OUTPUTS: \n",
    "#   1. Creates a detailed .txt validation report in the same directory as Excel results\n",
    "#   2. Adds the model's confusion matrix to an existing Excel file containing all models' confusion matrices\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"\\nVALIDATION: Creating confusion matrix for {selected_model_name}\")\n",
    "print(f\"Using all {total_features} features on {len(X)} records\")\n",
    "print(f\"This validates the per-class accuracy calculations\")\n",
    "\n",
    "# Use the same cross-validation setup as the main analysis\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Collect all predictions using identical methodology\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "print(f\"\\nRunning cross-validation...\")\n",
    "\n",
    "# Manual cross-validation (identical to main code)\n",
    "for train_idx, test_idx in cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Use fresh model instance and train/predict (same as main code)\n",
    "    validation_model = default_model_dict[selected_model_name]\n",
    "    validation_model.fit(X_train, y_train)\n",
    "    y_pred = validation_model.predict(X_test)\n",
    "    \n",
    "    # Store all predictions\n",
    "    y_true_all.extend(y_test.tolist())\n",
    "    y_pred_all.extend(y_pred.tolist())\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "\n",
    "# Create readable confusion matrix with class labels\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                     index=[f\"True_{cls}\" for cls in sorted(y.unique())], \n",
    "                     columns=[f\"Pred_{cls}\" for cls in sorted(y.unique())])\n",
    "\n",
    "# Calculate per-class accuracy from confusion matrix\n",
    "validation_per_class = {}\n",
    "for i, class_name in enumerate(sorted(y.unique())):\n",
    "    class_total = cm[i, :].sum()  # Total actual samples of this class\n",
    "    class_correct = cm[i, i]      # Correctly predicted samples\n",
    "    class_accuracy = class_correct / class_total if class_total > 0 else 0\n",
    "    validation_per_class[class_name] = class_accuracy\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = sum(y_true_all[i] == y_pred_all[i] for i in range(len(y_true_all))) / len(y_true_all)\n",
    "\n",
    "# Identify perfect and poor performing classes\n",
    "perfect_classes = [cls for cls, acc in validation_per_class.items() if acc == 1.0]\n",
    "poor_classes = [cls for cls, acc in validation_per_class.items() if acc < 0.8]\n",
    "\n",
    "# Generate validation report filename\n",
    "model_name_clean = selected_model_name.lower().replace(\"classifier\", \"\")\n",
    "validation_filename = f\"baseline_{model_name_clean}_validation_{min_features}_{total_features}.txt\"\n",
    "validation_path = results_dir / validation_filename\n",
    "\n",
    "# Create comprehensive text report\n",
    "report_content = []\n",
    "report_content.append(\"=\" * 80)\n",
    "report_content.append(f\"VALIDATION REPORT: {selected_model_name}\")\n",
    "report_content.append(\"=\" * 80)\n",
    "report_content.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "report_content.append(f\"Dataset: {len(X):,} records, {total_features} features\")\n",
    "report_content.append(f\"Validation uses all {total_features} features (complete feature set)\")\n",
    "#report_content.append(f\"Feature range tested: {min_features} to {total_features} features\")\n",
    "report_content.append(\"\")\n",
    "\n",
    "# Confusion Matrix Section\n",
    "report_content.append(\"CONFUSION MATRIX\")\n",
    "report_content.append(\"-\" * 40)\n",
    "report_content.append(\"Rows = True Bean Classes, Columns = Predicted Bean Classes\")\n",
    "report_content.append(f\"Bean Classes: {sorted(y.unique())}\")\n",
    "report_content.append(\"\")\n",
    "report_content.append(cm_df.to_string())\n",
    "report_content.append(\"\")\n",
    "\n",
    "# Per-Class Accuracy Section\n",
    "report_content.append(\"PER-CLASS ACCURACY (from confusion matrix)\")\n",
    "report_content.append(\"-\" * 50)\n",
    "for i, class_name in enumerate(sorted(y.unique())):\n",
    "    class_total = cm[i, :].sum()\n",
    "    class_correct = cm[i, i]\n",
    "    class_accuracy = validation_per_class[class_name]\n",
    "    report_content.append(f\"   Bean Class {class_name}: {class_accuracy:.4f} ({class_accuracy*100:.2f}%) - {class_correct}/{class_total} correct\")\n",
    "\n",
    "report_content.append(\"\")\n",
    "\n",
    "# Overall Accuracy Section\n",
    "report_content.append(\"OVERALL PERFORMANCE\")\n",
    "report_content.append(\"-\" * 30)\n",
    "report_content.append(f\"Overall Accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
    "report_content.append(\"\")\n",
    "\n",
    "# Performance Classification\n",
    "if perfect_classes:\n",
    "    report_content.append(f\"Perfect Classification (100%): Bean classes {perfect_classes}\")\n",
    "if poor_classes:\n",
    "    report_content.append(f\"Challenging Classification (<80%): Bean classes {poor_classes}\")\n",
    "if not perfect_classes and not poor_classes:\n",
    "    report_content.append(\"All classes achieved 80%+ accuracy, none achieved 100%\")\n",
    "\n",
    "report_content.append(\"\")\n",
    "\n",
    "# Comparison to Main Results\n",
    "report_content.append(\"COMPARISON TO MAIN BENCHMARK RESULTS\")\n",
    "report_content.append(\"-\" * 45)\n",
    "report_content.append(\"   ‚Ä¢ These per-class accuracies are for the complete 16-feature set only\")\n",
    "report_content.append(\"   ‚Ä¢ Your main benchmark Excel file shows averages across all 137 feature combinations\")\n",
    "report_content.append(\"   ‚Ä¢ Expected: These validation results may differ slightly from main benchmark averages\")\n",
    "report_content.append(\"   ‚Ä¢ This validation confirms the accuracy calculation methodology\")\n",
    "report_content.append(\"\")\n",
    "\n",
    "# Additional Statistics\n",
    "report_content.append(\"DETAILED STATISTICS\")\n",
    "report_content.append(\"-\" * 25)\n",
    "report_content.append(f\"Cross-validation folds: 5\")\n",
    "report_content.append(f\"Random state: 42 (reproducible results)\")\n",
    "report_content.append(f\"Total predictions made: {len(y_true_all):,}\")\n",
    "report_content.append(f\"Correct predictions: {sum(y_true_all[i] == y_pred_all[i] for i in range(len(y_true_all))):,}\")\n",
    "report_content.append(f\"Incorrect predictions: {len(y_true_all) - sum(y_true_all[i] == y_pred_all[i] for i in range(len(y_true_all))):,}\")\n",
    "\n",
    "# Class distribution\n",
    "report_content.append(\"\")\n",
    "report_content.append(\"CLASS DISTRIBUTION IN VALIDATION\")\n",
    "report_content.append(\"-\" * 35)\n",
    "for class_name in sorted(y.unique()):\n",
    "    class_count = y_true_all.count(class_name)\n",
    "    class_percentage = (class_count / len(y_true_all)) * 100\n",
    "    report_content.append(f\"   Bean Class {class_name}: {class_count:,} samples ({class_percentage:.1f}%)\")\n",
    "\n",
    "report_content.append(\"\")\n",
    "report_content.append(\"=\" * 80)\n",
    "report_content.append(\"END OF VALIDATION REPORT\")\n",
    "report_content.append(\"=\" * 80)\n",
    "\n",
    "# Write to file\n",
    "with open(validation_path, 'w') as f:\n",
    "    f.write('\\n'.join(report_content))\n",
    "\n",
    "# Display summary to console\n",
    "print(f\"\\nVALIDATION SUMMARY:\")\n",
    "print(f\"   Model: {selected_model_name}\")\n",
    "print(f\"   Overall Accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
    "if perfect_classes:\n",
    "    print(f\"   Perfect Classes: {perfect_classes}\")\n",
    "if poor_classes:\n",
    "    print(f\"   ‚ö†Ô∏è  Challenging Classes: {poor_classes}\")\n",
    "\n",
    "print(f\"\\nValidation report saved to: {validation_path}\")\n",
    "print(f\"Report includes confusion matrix, per-class accuracy, and detailed statistics\")\n",
    "print(f\"\\nValidation complete! All results saved to text file.\")\n",
    "\n",
    "\n",
    "# Save confusion matrix to Excel format for consistency analysis\n",
    "cm_excel_filename = f\"baseline_confusion_matrices_all_models_{min_features}_{total_features}.xlsx\"\n",
    "cm_excel_path = results_dir / cm_excel_filename\n",
    "\n",
    "# Add model name column to confusion matrix\n",
    "cm_df_excel = cm_df.copy()\n",
    "cm_df_excel.insert(0, 'Model', selected_model_name)\n",
    "\n",
    "# Check if Excel file already exists\n",
    "if cm_excel_path.exists():\n",
    "    # File exists - add new sheet to existing workbook\n",
    "    with pd.ExcelWriter(cm_excel_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "        cm_df_excel.to_excel(writer, sheet_name=selected_model_name, index=True)\n",
    "    print(f\"Added {selected_model_name} sheet to existing Excel file: {cm_excel_path}\")\n",
    "else:\n",
    "    # File doesn't exist - create new workbook with first sheet\n",
    "    cm_df_excel.to_excel(cm_excel_path, sheet_name=selected_model_name, index=True)\n",
    "    print(f\"Created new Excel file with {selected_model_name} sheet: {cm_excel_path}\")\n",
    "\n",
    "\n",
    "# Calculate and save F1-scores for this model\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1-scores using existing predictions from validation\n",
    "f1_macro = f1_score(y_true_all, y_pred_all, average='macro')\n",
    "f1_weighted = f1_score(y_true_all, y_pred_all, average='weighted')\n",
    "\n",
    "# Create F1-scores summary (single row)\n",
    "f1_summary = {\n",
    "    'Model': selected_model_name,\n",
    "    'F1_Macro': round(f1_macro, 4),\n",
    "    'F1_Weighted': round(f1_weighted, 4),\n",
    "    'Overall_Accuracy': round(overall_accuracy, 4)\n",
    "}\n",
    "\n",
    "# Create F1-scores DataFrame and save to Excel\n",
    "f1_df = pd.DataFrame([f1_summary])\n",
    "f1_filename = f\"baseline_{model_name_clean}_f1_scores_{min_features}_{total_features}.xlsx\"\n",
    "f1_path = results_dir / f1_filename\n",
    "f1_df.to_excel(f1_path, index=False)\n",
    "\n",
    "# Confirmation print with full path\n",
    "print(f\"F1-scores Excel file created and saved: {f1_path}\")\n",
    "print(f\"   F1-Macro: {f1_macro:.4f}, F1-Weighted: {f1_weighted:.4f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e2a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Consolidating F1-scores from 16 models...\n",
      "   Added adaboost F1-scores to consolidated file\n",
      "   Added decisiontree F1-scores to consolidated file\n",
      "   Added extratrees F1-scores to consolidated file\n",
      "   Added gaussiannb F1-scores to consolidated file\n",
      "   Added gradientboosting F1-scores to consolidated file\n",
      "   Added kneighbors F1-scores to consolidated file\n",
      "   Added lineardiscriminantanalysis F1-scores to consolidated file\n",
      "   Added logisticregression F1-scores to consolidated file\n",
      "   Added mlp F1-scores to consolidated file\n",
      "   Added perceptron F1-scores to consolidated file\n",
      "   Added quadraticdiscriminantanalysis F1-scores to consolidated file\n",
      "   Added randomforest F1-scores to consolidated file\n",
      "   Added ridge F1-scores to consolidated file\n",
      "   Added sgd F1-scores to consolidated file\n",
      "   Added svc F1-scores to consolidated file\n",
      "   Added xgb F1-scores to consolidated file\n",
      "\n",
      "‚úÖ F1-scores consolidated successfully: C:\\Misc\\ml_benchmark\\outputs\\baseline_results\\baseline_f1_scores_all_models_14_16.xlsx\n",
      "   Total sheets: 16\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 7. CONSOLIDATE F1-SCORES FROM ALL MODELS INTO SINGLE EXCEL FILE\n",
    "# -------------------------------------------\n",
    "\n",
    "# Run this module only after all ML models have been tested and the individual F1-scores\n",
    "# Excel files have been created in the prior code cells.  It will collect the data from\n",
    "# each model's F1-scores Excel file and consolidate them into a single Excel file\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define search pattern for the F1-score Excel files\n",
    "f1_pattern = f\"baseline_*_f1_scores_{min_features}_{total_features}.xlsx\"\n",
    "f1_files = list(results_dir.glob(f1_pattern))    # Uses the .glob() method of the pathlib.Path object, so glob import not needed\n",
    "\n",
    "# Create consolidated F1-scores Excel file\n",
    "consolidated_f1_filename = f\"baseline_f1_scores_all_models_{min_features}_{total_features}.xlsx\"\n",
    "consolidated_f1_path = results_dir / consolidated_f1_filename\n",
    "\n",
    "if f1_files:\n",
    "    print(f\"\\nConsolidating F1-scores from {len(f1_files)} models...\")\n",
    "    \n",
    "    with pd.ExcelWriter(consolidated_f1_path, engine='openpyxl') as writer:\n",
    "        for f1_file in f1_files:\n",
    "            # Extract model name from filename\n",
    "            model_name = f1_file.stem.replace(f\"baseline_\", \"\").replace(f\"_f1_scores_{min_features}_{total_features}\", \"\")\n",
    "            \n",
    "            # Read the F1-score data\n",
    "            f1_data = pd.read_excel(f1_file)\n",
    "            \n",
    "            # Write to sheet named after model\n",
    "            f1_data.to_excel(writer, sheet_name=model_name, index=False)\n",
    "            \n",
    "            print(f\"   Added {model_name} F1-scores to consolidated file\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ F1-scores consolidated successfully: {consolidated_f1_path}\")\n",
    "    print(f\"   Total sheets: {len(f1_files)}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  No F1-score files found matching pattern: {f1_pattern}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
