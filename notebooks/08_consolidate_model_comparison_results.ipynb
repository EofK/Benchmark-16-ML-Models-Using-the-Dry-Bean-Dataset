{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054ee893",
   "metadata": {},
   "source": [
    "Summarize the Results of the 07_compare_models program.\n",
    "\n",
    "1. Load the Excel file for each model from the prior module (where each model was compared on common feature sets)\n",
    "2. Match each row using a clean, sorted feature set key\n",
    "3. Merge all metrics into a single, unified table: one row for each unique set of dataset features\n",
    "4. Log any duplicate feature sets to a separate .txt file (there should not be any of these)\n",
    "5. Compute six summary accuracy stats for each row (e.g., mean, median, best model, worst model)\n",
    "6. Compute six summary runtime stats for each row (e.g., mean, meadian, fastest model, slowest model)\n",
    "7. Export everything to a single Excel file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd8344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging feature sets: 100%|██████████| 137/137 [00:00<00:00, 96965.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Summary written to C:\\Misc\\ml_benchmark\\outputs\\summary\\baseline_consolidated_results_features_14_16_3_loops_each_feature_set.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 1. CONSOLIDATE MODEL COMPARISON RESULTS\n",
    "# -------------------------------------------\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import config paths\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import RESULTS_DIR, SUMMARY_DIR\n",
    "\n",
    "# Define path to all ML models' Excel files, and name the output files created in this module.\n",
    "# To create the file for the baseline models, add \"baseline_\" as a prefix to the model name, below.\n",
    "# Else do not add \"baseline_\" to the model name to create the file for the benchmarked (after tuning hyperparameters) models.\n",
    "input_dir = RESULTS_DIR / \"features_14_16\"\n",
    "output_file = SUMMARY_DIR / \"baseline_consolidated_results_features_14_16_3_loops_each_feature_set.xlsx\"\n",
    "duplicate_log = SUMMARY_DIR / \"baseline_duplicates_log.txt\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "SUMMARY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Take the first 16 columns of each file (the features), remove missing or null\n",
    "# values (e.g., for 14 features, there will be two columns that are blank),\n",
    "# sort the remaning feature names alpha, then convet to a tuple for use as a dictionary key.\n",
    "# Why?  Different Excel files will have the same feature sets, but may be in a different order.\n",
    "# Not doing this would result in additional Excel rows with the same set of features. Sweet, right?\n",
    "def extract_sorted_key(row):\n",
    "    features = [f for f in row[:16] if pd.notna(f) and str(f).strip()]\n",
    "    return tuple(sorted(features))\n",
    "\n",
    "# Set up two data structures to collect data from each model's Excel file,and organize it for merging:\n",
    "#   1. master_data will store performance metrics for each model-feature combination\n",
    "#   2. feature_rows will store the actual feature names for each unique combination (e.g., \"A\", \"P\", . . . ).\n",
    "# Why? Prevents the same feature set from appearing more than once for one model.\n",
    "master_data = defaultdict(dict)\n",
    "feature_rows = {}\n",
    "\n",
    "# Scan all Excel files\n",
    "# In filename.startswith, insert \"baseline_\" as prefix, below, to collect all the baseline models Excel files. \n",
    "# Else, insert \"benchmark_\" to collect all the benchmark models Excel files.\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".xlsx\") and filename.startswith(\"baseline_\"):\n",
    "        model_name = filename.split(\"_\")[1]  # Extract model name from benchmark_modelname_results_14_16.xlsx\n",
    "        path = input_dir / filename\n",
    "        df = pd.read_excel(path)\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            key = extract_sorted_key(row)\n",
    "            if key in master_data[model_name]:\n",
    "                with open(duplicate_log, \"a\") as f:\n",
    "                    f.write(f\"Duplicate detected in model: {model_name}\\n\")\n",
    "                    f.write(f\"Feature set: {key}\\n\\n\")\n",
    "                continue  # Skip duplicate\n",
    "\n",
    "            master_data[model_name][key] = {\n",
    "                f\"Accuracy_{model_name}\": row[\"Accuracy\"],\n",
    "                f\"Runtime_{model_name}\": row[\"Runtime (ms)\"]\n",
    "            }\n",
    "            feature_rows.setdefault(key, list(row[:16]))\n",
    "\n",
    "# Merge into one DataFrame\n",
    "rows = []\n",
    "\n",
    "for key in tqdm(feature_rows, desc=\"Merging feature sets\"):\n",
    "    base_row = feature_rows[key]\n",
    "    row_data = {f\"Feature_{i+1}\": base_row[i] if i < len(base_row) else \"\" for i in range(16)}\n",
    "\n",
    "    for model in master_data:\n",
    "        metrics = master_data[model].get(key, {})\n",
    "        row_data.update(metrics)\n",
    "\n",
    "    rows.append((key, row_data))\n",
    "\n",
    "df_combined = pd.DataFrame([r[1] for r in rows])\n",
    "\n",
    "\n",
    "# Compute summary stats per row (per feature set), which will be columns.\n",
    "accuracy_cols = [col for col in df_combined.columns if col.startswith(\"Accuracy_\")]\n",
    "runtime_cols = [col for col in df_combined.columns if col.startswith(\"Runtime_\")]\n",
    "\n",
    "df_combined[\"Mean_Accuracy\"] = df_combined[accuracy_cols].mean(axis=1)\n",
    "df_combined[\"Median_Accuracy\"] = df_combined[accuracy_cols].median(axis=1)\n",
    "df_combined[\"Max_Accuracy\"] = df_combined[accuracy_cols].max(axis=1)\n",
    "df_combined[\"Best_Accuracy_Model\"] = df_combined[accuracy_cols].idxmax(axis=1).str.replace(\"Accuracy_\", \"\")\n",
    "df_combined[\"Min_Accuracy\"] = df_combined[accuracy_cols].min(axis=1)\n",
    "df_combined[\"Worst_Accuracy_Model\"] = df_combined[accuracy_cols].idxmin(axis=1).str.replace(\"Accuracy_\", \"\")\n",
    "\n",
    "df_combined[\"Mean_Runtime (ms)\"] = df_combined[runtime_cols].mean(axis=1)\n",
    "df_combined[\"Median_Runtime (ms)\"] = df_combined[runtime_cols].median(axis=1)\n",
    "df_combined[\"Max_Runtime (ms)\"] = df_combined[runtime_cols].max(axis=1)\n",
    "df_combined[\"Slowest_Model\"] = df_combined[runtime_cols].idxmax(axis=1).str.replace(\"Runtime_\", \"\")\n",
    "df_combined[\"Min_Runtime (ms)\"] = df_combined[runtime_cols].min(axis=1)\n",
    "df_combined[\"Fastest_Model\"] = df_combined[runtime_cols].idxmin(axis=1).str.replace(\"Runtime_\", \"\")\n",
    "\n",
    "# Export to Excel\n",
    "df_combined.to_excel(output_file, index=False)\n",
    "print(f\"\\nSummary written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8a98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading confusion matrices from: C:\\Misc\\ml_benchmark\\outputs\\results\\confusion_matrices_all_models_14_16.xlsx\n",
      "Output will be saved to: C:\\Misc\\ml_benchmark\\outputs\\summary\\consolidated_per_class_accuracy_features.xlsx\n",
      "\n",
      "Consolidated per-class accuracy saved to: C:\\Misc\\ml_benchmark\\outputs\\summary\\consolidated_per_class_accuracy_features.xlsx\n",
      "Models processed: 16\n",
      "Columns: ['Model', 'Accuracy_0', 'Accuracy_1', 'Accuracy_2', 'Accuracy_3', 'Accuracy_4', 'Accuracy_5', 'Accuracy_6']\n",
      "\n",
      "Sample of consolidated data:\n",
      "                        Model  Accuracy_0  Accuracy_1  Accuracy_2  Accuracy_3  Accuracy_4  Accuracy_5  Accuracy_6\n",
      "              RidgeClassifier    0.830560    0.967433    0.940491    0.881557    0.915456    0.929946    0.900228\n",
      "                   GaussianNB    0.807867    1.000000    0.899387    0.875353    0.954357    0.941786    0.871017\n",
      "QuadraticDiscriminantAnalysis    0.894100    1.000000    0.952147    0.898195    0.955913    0.941786    0.880501\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 2. CONSOLIDATE PER-CLASS ACCURACY FROM CONFUSION MATRICES\n",
    "# -------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the confusion matrices Excel file (input)\n",
    "conf_matrix_file = Path(RESULTS_DIR) / \"confusion_matrices_all_models_14_16.xlsx\"\n",
    "# Path to output consolidated per-class accuracy file (no _14_16 suffix)\n",
    "output_file = Path(SUMMARY_DIR) / \"consolidated_per_class_accuracy_features.xlsx\"\n",
    "\n",
    "print(f\"Reading confusion matrices from: {conf_matrix_file}\")\n",
    "print(f\"Output will be saved to: {output_file}\")\n",
    "\n",
    "# Read all sheets (one per model)\n",
    "all_sheets = pd.read_excel(conf_matrix_file, sheet_name=None)\n",
    "\n",
    "consolidated = []\n",
    "\n",
    "for model_name, df in all_sheets.items():\n",
    "    # The actual confusion matrix is columns C onward (skip first two columns)\n",
    "    # Reset index in case there are any issues with row labels\n",
    "    matrix = df.iloc[:, 2:].apply(pd.to_numeric, errors='coerce')\n",
    "    per_class_acc = {}\n",
    "    for i in range(matrix.shape[0]):\n",
    "        true_positives = matrix.iloc[i, i]\n",
    "        total = matrix.iloc[i, :].sum()\n",
    "        acc = true_positives / total if total > 0 else float('nan')\n",
    "        per_class_acc[f\"Accuracy_{i}\"] = acc\n",
    "    row = {\"Model\": model_name}\n",
    "    row.update(per_class_acc)\n",
    "    consolidated.append(row)\n",
    "\n",
    "df_out = pd.DataFrame(consolidated)\n",
    "df_out.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"\\nConsolidated per-class accuracy saved to: {output_file}\")\n",
    "print(f\"Models processed: {len(df_out)}\")\n",
    "print(f\"Columns: {list(df_out.columns)}\")\n",
    "print(\"\\nSample of consolidated data:\")\n",
    "print(df_out.head(3).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
