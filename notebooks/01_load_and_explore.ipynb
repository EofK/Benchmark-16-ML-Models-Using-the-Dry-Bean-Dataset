{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 01_load_and_explore.ipynb\n",
    "# Author: Ed | Purpose: Benchmark ML models on a dataset\n",
    "# -------------------------------------------\n",
    "\n",
    "# -------------------------------------------\n",
    "# 1. CONFIGURE AND LOAD THE DATASET\n",
    "# -------------------------------------------\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------------------\n",
    "# CENTRALIZED CONFIGURATION SYSTEM\n",
    "# -------------------------------------------\n",
    "# This project is multiple Jupyter notebooks (Python code), with a pre-defined directory (folders) \n",
    "# structure, and uses a centralized config.py file for all path management.\n",
    "#\n",
    "# Benefits may include:\n",
    "# - Single source of truth for all project paths\n",
    "# - Easy to change project location (update config.py only)\n",
    "# - Consistent paths across all pipeline stages (Python modules)\n",
    "# - Professional project structure\n",
    "# \n",
    "# The config.py file is located in the project root directory and contains\n",
    "# all directory paths and file paths.\n",
    "# -------------------------------------------\n",
    "\n",
    "\n",
    "# Import config paths\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import DATASETS_DIR, CLEAN_DATA_DIR\n",
    "\n",
    "# Set dataset parameters here to work with different datasets.\n",
    "# Change the dataset name, the dataset's Excel (or .csv) file name with extension,\n",
    "# label column, and any columns to force categorical or to drop.\n",
    "# This allows for reuse of the code across different datasets.\n",
    "\n",
    "DATASET_NAME = \"DryBean\"\n",
    "DATASET_PATH = DATASETS_DIR / \"Dry_Bean_Dataset.xlsx\"\n",
    "\n",
    "LABEL_COLUMN = \"Class\"  # <-- change if using another dataset\n",
    "# ID which cols are skipped from numeric diagnostics, or dropped.\n",
    "FORCE_CATEGORICAL = []  # Optional. E.g. ['Zip', 'ProductID']. \n",
    "DROP_COLUMNS = []       # Optional. E.g. ['ID']\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    if str(path).endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "    elif str(path).endswith((\".xls\", \".xlsx\")):\n",
    "        df = pd.read_excel(path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format.\")\n",
    "    return df\n",
    "\n",
    "df = load_dataset(DATASET_PATH)\n",
    "print(f\"Loaded dataset: {DATASET_NAME}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584971f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 2. DETERMINE FEATURES AND LABEL(S) TYPES\n",
    "# -------------------------------------------\n",
    "\n",
    "# Function to detect numerical and categorical columns.\n",
    "# Written to allow identifying features (column names) that should be treated as categorical, \n",
    "# even if they contain only numeric values. (E.g. ZipCode, ProductID). -- Not the case with Dry Bean dataset.\n",
    "def detect_column_types(df, label_column, force_categoricals=None):\n",
    "    numericals = df.select_dtypes(include=[\"int\", \"float\"]).columns.tolist()\n",
    "    categoricals = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "    # Validate and apply forced categoricals\n",
    "    if force_categoricals:\n",
    "        print(\"Validating FORCED CATEGORICAL columns...\")\n",
    "        for col in force_categoricals:\n",
    "            if col not in df.columns:\n",
    "                print(f\"‚ö†Ô∏è Warning: '{col}' not found in dataset and will be ignored.\")\n",
    "            else:\n",
    "                if col not in categoricals:\n",
    "                    categoricals.append(col)\n",
    "                if col in numericals:\n",
    "                    numericals.remove(col)\n",
    "\n",
    "    # Remove label column from both lists\n",
    "    if label_column in numericals:\n",
    "        numericals.remove(label_column)\n",
    "    if label_column in categoricals:\n",
    "        categoricals.remove(label_column)\n",
    "\n",
    "    return sorted(numericals), sorted(categoricals)\n",
    "\n",
    "# Display which columns are numerical and categorical\n",
    "numericals, categoricals = detect_column_types(df, LABEL_COLUMN, FORCE_CATEGORICAL)\n",
    "\n",
    "print(f\"Numerical columns ({len(numericals)}): {numericals}\")\n",
    "print(f\"Categorical columns ({len(categoricals)}): {categoricals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184940b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 3. ASSESS THE DATA   \n",
    "# -------------------------------------------\n",
    "\n",
    "# Drop columns identified in prior module. Confirm each column exists in the DataFrame\n",
    "if DROP_COLUMNS:\n",
    "    existing_to_drop = [col for col in DROP_COLUMNS if col in df.columns]\n",
    "    if existing_to_drop:\n",
    "        df.drop(columns=existing_to_drop, inplace=True)\n",
    "        print(f\"üóëÔ∏è Dropped columns: {existing_to_drop}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è None of the specified DROP_COLUMNS were found in dataset.\")\n",
    "\n",
    "# Print data types and missing values\n",
    "display(df.info())\n",
    "display(df.describe(include=\"all\").T)\n",
    "\n",
    "# Count and display missing values in each column\n",
    "nulls = df.isnull().sum()\n",
    "print(\"Null values by column:\")\n",
    "display(nulls[nulls > 0])\n",
    "\n",
    "# Reveal if there are balanced or imbalanced classes in the label column\n",
    "if LABEL_COLUMN in df.columns:\n",
    "    print(\"Label distribution (Count and Percentage):\")\n",
    "    \n",
    "    # Get both counts and percentages\n",
    "    counts = df[LABEL_COLUMN].value_counts().sort_index()\n",
    "    percentages = df[LABEL_COLUMN].value_counts(normalize=True).mul(100).round(2).sort_index()\n",
    "    \n",
    "    # Create a combined table\n",
    "    distribution_df = pd.DataFrame({\n",
    "        'Count': counts,\n",
    "        'Percentage': percentages\n",
    "    })\n",
    "    \n",
    "    display(distribution_df)\n",
    "    \n",
    "    # Also show total for verification\n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Verification: {distribution_df['Count'].sum():,} records\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: Label column not found.\")\n",
    "\n",
    "\n",
    "# Calculate coefficient of variation for numerical features\n",
    "if numericals:\n",
    "    cv_stats = df[numericals].std() / df[numericals].mean()\n",
    "    cv_df = pd.DataFrame({\n",
    "        'Feature': cv_stats.index,\n",
    "        'Coefficient_of_Variation': cv_stats.values\n",
    "    }).sort_values('Coefficient_of_Variation', ascending=False)\n",
    "    \n",
    "    print(\"Coefficient of Variation (CV) for numerical features:\")\n",
    "    print(\"(Higher CV = more relative variability)\")\n",
    "    display(cv_df)    \n",
    "\n",
    " # Verify whether Solidity truly shows high within-class spread for Sira and Dermason\n",
    "print(\"\\nVerifying Solidity spread for Sira and Dermason:\")\n",
    "df[df['Class'].isin(['SIRA', 'DERMASON'])].groupby('Class')['S'].agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855bed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 4. CREATE FEATURE DIAGNOSTICS VISUALS\n",
    "# -------------------------------------------\n",
    "\n",
    "# Prepare histograms for numeric features\n",
    "df[numericals].hist(figsize=(16, 10), bins=30)\n",
    "plt.suptitle(\"Distribution of Numerical Features\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare the correlation matrix\n",
    "corr = df[numericals].corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Correlation Matrix\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Display the number of records for each bean type, as one way to show class imbalance.\n",
    "if LABEL_COLUMN in df.columns:\n",
    "    sns.countplot(x=df[LABEL_COLUMN])\n",
    "    plt.title(\"Target Label Distribution\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Make it easy to select pairs of features to display pair plots.\n",
    "# Comes in handy later for exploring/explaining accuracy results. \n",
    "import plotly.express as px\n",
    "\n",
    "for x, y in [('P', 'CO'), ('S', 'SF2'), ('l', 'SF3')]:\n",
    "    fig = px.scatter(df, x=x, y=y, color='Class')\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Results of later modules in this pipeline found that the most common\n",
    "# bean incorrectly labeled by the models was Sira.  And nearly\n",
    "# 75% of these errors classified the Sira bean as a Dermason.\n",
    "#\n",
    "# The code below attempts to determine why.\n",
    "#\n",
    "# The code below identifies feature pairs that have the strongest combination of:\n",
    "#    1. Proximity in feature space (small Euclidean distance between means)\n",
    "#    2. Large within-class dispersion (large standard deviation across both bean types).\n",
    "# \n",
    "# This means the two classes are not only close together, \n",
    "# but diffuse enough to bleed into each other‚Äôs zones, a prime \n",
    "# condition for misclassification.\n",
    "\n",
    "# Then, displaying a pair plot of the pairs identified, show a visual \n",
    "# jumble or strong overlap between Sira and Dermason dots,\n",
    "# making it clear why models struggled to tell them apart.\n",
    "\n",
    "\n",
    "\n",
    "# Identify all 16 features \n",
    "features = [\"A\", \"P\", \"L\", \"l\", \"K\", \"Ec\", \"C\", \"Ed\", \"Ex\", \"S\", \"R\", \"CO\", \"SF1\", \"SF2\", \"SF3\", \"SF4\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for f1 in features:\n",
    "    for f2 in features:\n",
    "        if f1 == f2: continue\n",
    "\n",
    "        # Subset\n",
    "        subset = df[df['Class'].isin(['SIRA', 'DERMASON'])]\n",
    "\n",
    "        # Get means\n",
    "        m_sira = subset[subset['Class'] == 'SIRA'][[f1, f2]].mean()\n",
    "        m_derm = subset[subset['Class'] == 'DERMASON'][[f1, f2]].mean()\n",
    "\n",
    "        # Euclidean distance between centroids\n",
    "        dist = ((m_sira - m_derm) ** 2).sum() ** 0.5\n",
    "\n",
    "        # Sum of standard deviations\n",
    "        sd_sira = subset[subset['Class'] == 'SIRA'][[f1, f2]].std().sum()\n",
    "        sd_derm = subset[subset['Class'] == 'DERMASON'][[f1, f2]].std().sum()\n",
    "        total_sd = sd_sira + sd_derm\n",
    "\n",
    "        results.append((f1, f2, dist, total_sd))\n",
    "\n",
    "# Sort by: small centroid distance, high variance\n",
    "ranked = sorted(results, key=lambda x: (x[2], -x[3]))\n",
    "\n",
    "\n",
    "# Display the list of the top 10 pairs, showing:\n",
    "#   1. Feature names\n",
    "#   2. How close the class centroids are\n",
    "#   3. How widely spread the data are for those features.\n",
    "\n",
    "# Print the top 10 most confusion-prone pairs\n",
    "print(\"Top 10 feature pairs with high Sira‚ÄìDermason overlap:\\n\")\n",
    "for i, (f1, f2, dist, sd) in enumerate(ranked[:10], start=1):\n",
    "    print(f\"{i}. {f1} vs {f2} | Centroid Distance: {dist:.4f} | Combined Std Dev: {sd:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea0c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 5. EXPORT COLUMN LISTS\n",
    "# -------------------------------------------\n",
    "\n",
    "# Save lists of features for use in the next module for feature-specific preprocessing\n",
    "\n",
    "#### out_dir = Path(\"C:/Misc/ml_benchmark/outputs/clean_data/\")\n",
    "out_dir = CLEAN_DATA_DIR\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Log all feature names that are numericals, and all that are categoricals.\n",
    "# Aids in how each might be preprocessed later (e.g., scale a numeric, encode a categorial).\n",
    "pd.Series(numericals).to_csv(out_dir / f\"{DATASET_NAME}_numericals.txt\", index=False, header=False)\n",
    "pd.Series(categoricals).to_csv(out_dir / f\"{DATASET_NAME}_categoricals.txt\", index=False, header=False)\n",
    "\n",
    "print(f\"Feature lists saved to: {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d13734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 6. SAVE DATAFRAME FOR USE IN OTHER NOTEBOOKS\n",
    "# -------------------------------------------\n",
    "\n",
    "# Export working dataset (dataframe df) for downstream reuse, or use in other workbooks.\n",
    "# NOTE: Requires pyarrow or fastparquet. Install via: pip install pyarrow\n",
    "\n",
    "df.to_parquet(CLEAN_DATA_DIR / \"DryBean_clean.parquet\")\n",
    "\n",
    "print(\"Saved cleaned dataframe to .parquet for reuse\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
